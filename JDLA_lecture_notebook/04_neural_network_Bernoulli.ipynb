{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークによる2クラス分類\n",
    "\n",
    "---\n",
    "## 目的\n",
    "多層パーセプトロン（Multi Layer Perceptron; MLP）を用いて，乳ガンデータの2クラス分類を行う．\n",
    "\n",
    "## 対応するチャプター\n",
    "* 6.2.2: ベルヌーイ分布出力のためのシグモイドユニット\n",
    "* 8.1.3: バッチアルゴリズムとミニバッチアルゴリズム\n",
    "* 8.3.1: 確率的勾配降下法\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．\n",
    "実験にはPythonの機械学習用ライブラリであるscikit-learnと深層学習ライブラリの一つであるchainerを使用します．\n",
    "使用するクラス，関数は以下の通りです．\n",
    "\n",
    "* `numpy`（説明は割愛）\n",
    "* `load_breast_cancer`はデータセットを読み込むための関数\n",
    "* `train_test_split`はデータを学習用とテスト用のデータに分割するための関数\n",
    "* `chainer`は深層学習を使用するためのPythonライブラリ\n",
    "* `chainer.cuda`はchainer上でGPUを使用した計算を行うためのモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import chainer\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "実験に使用するデータセットを読み込みます．\n",
    "\n",
    "今回はscikit-learnが提供する`load_breast_cancer`関数を用いて，データを読み込みます．\n",
    "breast cancer datasetは肺癌のデータセットであり，クラス数は悪性腫瘍 (malignant)と良性腫瘍 (benign) の2クラス，データ数は569（悪性腫瘍 (malignant): 220, 良性腫瘍 (benign): 357）のデータセットです．\n",
    "各データは細胞核の半径や面積，テクスチャ情報を表現した30次元のベクトルデータです．\n",
    "\n",
    "読み込んだ全てのデータのうち，`breast_cancer_data.data`でデータを読み込み，`breast_cancer_data.target`で教師ラベルをそれぞれ読み込みます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data = load_breast_cancer()\n",
    "x = breast_cancer_data.data.astype(np.float32)\n",
    "y = breast_cancer_data.target.astype(np.int32)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの分割と正規化\n",
    "上記で読み込んだデータを学習用データとテストデータに分割し，正規化を行います．\n",
    "\n",
    "データの分割には`train_test_split`関数を使用します．\n",
    "はじめに`test_sample_ratio`でテストに用いるデータ数の割合を指定します．\n",
    "その後，`train_test_split`関数を用いて指定した割合でデータを分割します．\n",
    "`random_state`はデータをランダムに分割する際のseedです．\n",
    "seedを変更，または指定しないことで，無作為にデータを分割することが可能です．\n",
    "\n",
    "次に正規化を行います．\n",
    "データ$x$の最小値を$x_{min}$，最大値を$x_{max}$としたとき，次の式で正規化を行います．\n",
    "$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "`np.min`と`np.max`で学習データの最大，最小値を取得し，上記の式に従い0~1の範囲に値を正規化します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの割合を指定\n",
    "test_sample_ratio = 0.2\n",
    "\n",
    "# データを分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_sample_ratio, random_state=0)\n",
    "\n",
    "# データの正規化\n",
    "x_min = np.min(x_train, axis=0)\n",
    "x_max = np.max(x_train, axis=0)\n",
    "\n",
    "x_train = (x_train[:, ] - x_min) / (x_max - x_min)\n",
    "x_test = (x_test[:, ] - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "ニューラルネットワーク（多層パーセプトロン）を定義します．\n",
    "ここでは，入力層，出力層から構成される2層の多層パーセプトロンを定義します．\n",
    "\n",
    "入力層のユニット数は入力データのサイズによります．\n",
    "ここではNoneとし，データにより変更できるようにしておきます．\n",
    "\n",
    "中間層と出力層のユニット数は引数として与え，それぞれ`n_hidden`，`n_out`とします．\n",
    "Chainerでは`__init__`関数にこれらの引数を与えて各層を定義します．\n",
    "各層の定義には`Linear`関数を使用します．\n",
    "これは全結合層を意味しています．\n",
    "\n",
    "そして，`__call__`関数で，定義した層を接続して処理するように記述します．\n",
    "`__call__`関数の引数`x`は入力データです．\n",
    "それを`__init__`関数で定義した`l1`という層に与え，その出力を活性化関数である`tanh`関数に入力します．\n",
    "その出力を`h`としています．\n",
    "`h`はさらに`l2`層に入力され，最終的な結果を出力します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_hidden, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_hidden)\n",
    "            self.l2 = L.Linear(n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.tanh(self.l1(x))\n",
    "        h = self.l2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの作成と学習の準備\n",
    "上のプログラムで定義したネットワークを作成します．\n",
    "\n",
    "\n",
    "まず，中間層と出力層のユニット数を定義します．\n",
    "ここでは，中間層のユニット数`n_units`128，出力層のユニット数`out_units`を1とします．\n",
    "\n",
    "各層のユニット数を`MLP`クラスの引数として与え，ネットワークを作成します．\n",
    "\n",
    "学習を行う際の最適化方法としてSGD (確率的勾配降下法）を利用します．また，学習率を0.001として引数に与えます．そして，最適化方法のsetup関数にネットワークモデルを与えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニット数の定義\n",
    "hidden_units = 128\n",
    "out_units = 1\n",
    "\n",
    "# ネットワークの作成\n",
    "model = MLP(n_hidden=hidden_units, n_out=out_units)\n",
    "\n",
    "# 最適化手法の設定\n",
    "optimizer = chainer.optimizers.SGD(lr=0.001)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "読み込んだbreast cancerデータセットと作成したネットワークを用いて，学習を行います．\n",
    "\n",
    "1回の誤差を算出するデータ数（ミニバッチサイズ）を10，学習エポック数を200とします．\n",
    "\n",
    "学習データは毎回ランダムに決定するため，numpyの`permutation`という関数を利用します．\n",
    "各更新において，学習用データと教師データをそれぞれ`x`と`t`とします．\n",
    "学習モデルに`x`を与えて各クラスの確率`y`を取得します．\n",
    "各クラスの確率`y`と教師ラベル`t`との誤差を`sigmoid_coross_entropy`誤差関数で算出します．\n",
    "また，認識精度も算出します．\n",
    "そして，誤差を`backward`関数で逆伝播し，ネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチサイズ・エポック数・学習データ数の設定\n",
    "batch_size = 10\n",
    "epoch_num = 200\n",
    "train_data_num = x_train.shape[0]\n",
    "\n",
    "# 学習の実行\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    sum_loss = 0.0\n",
    "    sum_accuracy = 0.0\n",
    "    \n",
    "    perm = np.random.permutation(train_data_num)\n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        x = Variable(x_train[perm[i:i+batch_size]])\n",
    "        t = Variable(y_train[perm[i:i+batch_size]].reshape(-1, 1))\n",
    "        t_acc = Variable(y_train[perm[i:i+batch_size]])\n",
    "        \n",
    "        model.zerograds()\n",
    "        y = model(x)\n",
    "\n",
    "        # 精度の計算\n",
    "        pred = F.concat((1 - F.sigmoid(y), F.sigmoid(y)), axis=1)\n",
    "        acc = F.accuracy(pred, t_acc)\n",
    "        \n",
    "        loss = F.sigmoid_cross_entropy(y, t)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        \n",
    "        sum_loss += loss.data * batch_size\n",
    "        sum_accuracy += acc.data * batch_size\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch,\n",
    "                                                                   sum_loss/train_data_num,\n",
    "                                                                   sum_accuracy/train_data_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "学習したネットワークを用いて，テストデータに対する認識率の確認を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "test_data_num = x_test.shape[0]\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "\n",
    "    for i in range(test_data_num):\n",
    "        x = Variable(np.array([x_test[i]], dtype=np.float32))\n",
    "        t = y_test[i]\n",
    "        y = F.sigmoid(model(x))\n",
    "\n",
    "        if y.data[0, 0] > 0.5:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "\n",
    "        if pred == t:\n",
    "            count += 1\n",
    "\n",
    "print(\"test accuracy: {}\".format(count / test_data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
