{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークによるMNISTデータセットの認識実験\n",
    "\n",
    "---\n",
    "## 目的\n",
    "多層パーセプトロン (Multi Layer Perceptoron; MLP) を用いてMNISTデータセットに対する文字認識を行う．\n",
    "また，使用する活性化関数の違いによる認識性能の変化を確認する．\n",
    "\n",
    "## 対応するチャプター\n",
    "* 6.2.2: マルチヌーイ分布出力のためのソフトマックスユニット\n",
    "* 6.3: ReLUとその一般化\n",
    "* 8.1.3: バッチアルゴリズムとミニバッチアルゴリズム\n",
    "* 8.3.1: 確率的勾配降下法\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．\n",
    "実験には深層学習ライブラリの一つであるchainerを使用します．\n",
    "使用するクラス，関数は以下の通りです．\n",
    "\n",
    "* `chainer`は深層学習を使用するためのPythonライブラリ\n",
    "* `get_mnist`はMNISTデータセットを使用するための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import get_mnist\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUの確認\n",
    "GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "`GPU avilability: True`と表示されれば，GPUを使用した計算をChainerで行うことが可能です．\n",
    "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "学習データ（MNISTデータセット）を読み込みます．\n",
    "\n",
    "MNISTデータセットの読み込みには`get_mnist`関数を使用します．\n",
    "読み込んだ学習データのサイズを確認します．\n",
    "学習データ数は6万枚．1つのデータのサイズは786次元となっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = chainer.datasets.get_mnist()\n",
    "train_x, train_y = train_dataset._datasets\n",
    "test_x, test_y = test_dataset._datasets\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNISTデータセットの表示\n",
    "MNISTデータセットに含まれる画像を表示してみます．\n",
    "ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure(figsize=(14, 1.4))\n",
    "for c in range(10):\n",
    "    ax = fig.add_subplot(1, 10, c + 1)\n",
    "    ax.imshow(train_x[c].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "ニューラルネットワークを定義します．\n",
    "ここでは，入力層，中間層，出力層から構成される3層のニューラルネットワークとします．\n",
    "\n",
    "入力層のユニット数は入力データのサイズによります．\n",
    "ここでは`None`とし，データにより変更できるようにしておきます．\n",
    "\n",
    "中間層と出力層のユニット数は引数として与え，`n_hidden`，`n_out`とします．\n",
    "Chainerでは，`__init__`関数にこれらの引数を与えて各層を定義します．\n",
    "各層は`Linear`関数としています．これは全結合層を意味しています．\n",
    "\n",
    "そして，`__call__`関数で定義した層を接続して処理するように記述します．\n",
    "`__call__`関数の引数`x`は入力データです．\n",
    "それを`__init__`関数で定義した`l1`という中間層に与え，その出力を活性化関数である`sigmoid`関数に与えます．\n",
    "その出力を`h`としています．\n",
    "`h`はさらに`l2`層，`l3`層へ入力することで，最終的な結果を出力します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_hidden, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_hidden)\n",
    "            self.l2 = L.Linear(n_hidden, n_hidden)\n",
    "            self.l3 = L.Linear(n_hidden, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # 課題2: 活性化関数の変更\n",
    "        # h = F.relu(self.l1(x))        # ReLUを使用する場合\n",
    "        # h = F.leaky_relu(self.l1(x))  # leaky ReLUを使用する場合\n",
    "        \n",
    "        h = F.sigmoid(self.l1(x))\n",
    "        h = F.sigmoid(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの作成と学習の準備\n",
    "上のプログラムで定義したネットワークを作成します．\n",
    "\n",
    "\n",
    "まず，中間層と出力層のユニット数を定義します．\n",
    "ここでは，中間層のユニット数`n_units`16，出力層のユニット数`out_units`を10とします．\n",
    "\n",
    "各層のユニット数を`MLP`クラスの引数として与え，ネットワークを作成します．\n",
    "\n",
    "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．\n",
    "また，学習率を0.01，モーメンタムを0.9として引数に与えます．\n",
    "そして，最適化方法のsetup関数にネットワークモデルを与えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニット数の定義\n",
    "hidden_units = 64\n",
    "out_units = 10\n",
    "\n",
    "# ネットワークの作成\n",
    "model = MLP(n_hidden=hidden_units, n_out=out_units)\n",
    "\n",
    "# 最適化手法の設定\n",
    "optimizer = chainer.optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "読み込んだMNISTデータセットと作成したネットワークを用いて，学習を行います．\n",
    "\n",
    "1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を10とします．\n",
    "\n",
    "学習を開始します．\n",
    "MNISTの学習データサイズを取得し，1エポック内における更新回数を求めます．\n",
    "学習データは毎エポックでランダムに利用するため，numpyの`permutation`という関数を利用します．\n",
    "各更新において，学習用データと教師データをそれぞれ`x`と`t`とします．\n",
    "学習モデルに`x`を与えて各クラスの確率`y`を取得します．\n",
    "各クラスの確率`y`と教師ラベル`t`との誤差を`softmax_coross_entropy`誤差関数で算出します．\n",
    "また，認識精度も算出します．\n",
    "そして，誤差を`backward`関数で逆伝播し，ネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "train_data_num = train_x.shape[0]\n",
    "\n",
    "# 学習の実行\n",
    "for epoch in range(epoch_num):\n",
    "    sum_loss = 0\n",
    "    sum_accuracy = 0\n",
    "    perm = np.random.permutation(train_data_num)\n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        x = Variable(train_x[perm[i:i+batch_size]])\n",
    "        t = Variable(train_y[perm[i:i+batch_size]])\n",
    "\n",
    "        model.zerograds()\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "        acc = F.accuracy(y, t)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "    \n",
    "        sum_loss += loss.data*batch_size\n",
    "        sum_accuracy += acc.data*batch_size\n",
    "    \n",
    "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch+1,\n",
    "                                                               sum_loss/train_data_num,\n",
    "                                                               sum_accuracy/train_data_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "学習したネットワークを用いて，テストデータに対する認識率の確認を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "test_data_num = test_x.shape[0]\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    for i in range(test_data_num):\n",
    "        x = Variable(np.array([test_x[i]], dtype=np.float32))\n",
    "        t = test_y[i]\n",
    "        y = model(x)\n",
    "        y =np.argmax(y.data[0])\n",
    "        if t == y:\n",
    "            count += 1\n",
    "    \n",
    "    print(\"test accuracy: {}\".format(count/test_data_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "1. ネットワークの中間層のユニット数を変更して認識性能の変化を確認しましょう\n",
    "2. 活性化関数を変更して認識性能の変化を確認しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
