{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回帰結合型のニューラルネットワークによる文章生成\n",
    "\n",
    "---\n",
    "## 目的\n",
    "回帰結合型のニューラルネットワーク，すなわち再帰型ニューラルネットワーク (Recurrent Neural Network; RNN) を用いてPenn Tree Bankデータセットに対する次単語の予測を行う．\n",
    "また，教師強制の有無による性能の違いを確認する．\n",
    "\n",
    "\n",
    "## 対応するチャプター\n",
    "* 10.2: 教師強制と出力回帰のあるネットワーク\n",
    "* 10.2: 回帰結合がたネットワークにおける勾配計算（BPTT）\n",
    "* 10.10.1: LSTM\n",
    "* 10.10.2: GRU\n",
    "* 10.11: 勾配のクリッピング\n",
    "\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import get_ptb_words, get_ptb_words_vocabulary\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.optimizer_hooks import GradientClipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUの確認\n",
    "GPUを使用した計算が可能かどうかを確認します．\n",
    "\n",
    "`GPU avilability: True`と表示されれば，GPUを使用した計算をChainerで行うことが可能です．\n",
    "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability: True\n",
      "cuDNN availablility: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "\n",
    "Penn Tree Bank (PTB) データセットを読み込みます．\n",
    "\n",
    "読み込んだ学習データのサイズを確認します．\n",
    "学習，検証，テストデータはそれぞれ929589，73760，82430のサイズの1次元配列になっていることがわかります．\n",
    "\n",
    "また，`get_ptb_words_vocabulary`関数を用いて，ptbデータセットに存在する英単語の情報を取得します．\n",
    "`vocab`には英単語とその単語を示すIDが辞書型のオブジェクトとして格納されています．\n",
    "英単語の数は10000です．\n",
    "\n",
    "最後に，keyと値の組み合わせを逆にした辞書`inverse_vocab`を作成します．\n",
    "これはIDで出力された予測結果から英単語を検索する際に使用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929589,) (73760,) (82430,)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "train, val, test = get_ptb_words()\n",
    "print(train.shape, val.shape, test.shape)\n",
    "\n",
    "# 単語（vocabulary）の確認\n",
    "vocab = get_ptb_words_vocabulary()\n",
    "print(len(vocab))\n",
    "\n",
    "# 逆引きの辞書を作成\n",
    "inverse_vocab = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benn Tree Bankデータセットの表示\n",
    "\n",
    "PTBデータセットの中身を`print`関数を使って表示してみます．\n",
    "\n",
    "学習用データを表示すると，1次元配列に整数値が格納されていることがわかります．\n",
    "\n",
    "また，`vocab`のうち，英単語を指定すると，各英単語に対応するIDガ表示されます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentence: [ 0  1  2 ... 39 26 24]\n",
      "2107 1203 8702 4237 5934\n"
     ]
    }
   ],
   "source": [
    "print(\"train sentence:\", train)\n",
    "print(vocab['player'], vocab['primarily'], vocab['arose'], vocab['generate'], vocab['partnership'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "再帰型ニューラルネットワークを定義します．\n",
    "\n",
    "ここでは，埋め込み層1層，LSTM層1層，全結合層1層から構成されるネットワークとします．\n",
    "\n",
    "`reset_state`関数では，LSTM層が持つ，内部状態（隠れ状態・セル状態）を初期化します．\n",
    "\n",
    "次に，`__call__`関数では，定義した層を接続して処理するように記述します．\n",
    "`__call__`関数の引数`x`は入力データ（単語のID）です．\n",
    "入力データは`embed`にて，入力された単語のIDから入力された単語を表現するベクトルを生成します．\n",
    "その後，LSTM，全結合層へと入力することで，入力された単語の次の単語を予測結果として出力します．\n",
    "その際，LSTMおよび全結合層からの出力にはdropoutを適用しており，過学習の抑制を図っています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units):\n",
    "        super(RNNLM, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed = L.EmbedID(n_vocab, n_units)\n",
    "            self.l1 = L.LSTM(n_units, n_units)\n",
    "            self.l2 = L.Linear(n_units, n_vocab)\n",
    "\n",
    "        for param in self.params():\n",
    "            param.array[...] = np.random.uniform(-0.1, 0.1, param.shape)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0))\n",
    "        y = self.l2(F.dropout(h1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Dwuvfouzmd7"
   },
   "source": [
    "## ネットワークの作成\n",
    "上のプログラムで定義したネットワークを作成します．\n",
    "ここでは，GPUで学習を行うために，modelをGPUに送るto_gpu関数を利用しています．\n",
    "\n",
    "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．また，学習率を1.0として引数に与えます．そして，最適化方法のsetup関数にネットワークモデルを与えます．\n",
    "\n",
    "また，勾配の爆発により学習の不安定性に対応するため，勾配のクリッピングを行います．\n",
    "最適化手法を設定した`optimizer_1`に`add_hook`メソッドを用いて学習を行う際の条件を追加します．\n",
    "ここでは，勾配のクリッピングを行う`GradientClipping`関数を追加します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1562840288195,
     "user": {
      "displayName": "Tsubasa Hirakawa",
      "photoUrl": "https://lh5.googleusercontent.com/-p6Kjr3nd0AU/AAAAAAAAAAI/AAAAAAAAJG0/tCF9JFOo7tk/s64/photo.jpg",
      "userId": "03545166870843244307"
     },
     "user_tz": -540
    },
    "id": "23m79Eq-zmjl",
    "outputId": "3973baec-fcdd-4766-e766-49da6921010f"
   },
   "outputs": [],
   "source": [
    "num_vocab = len(vocab)\n",
    "num_units = 1024\n",
    "model_1 = RNNLM(n_vocab=num_vocab, n_units=num_units)\n",
    "model_1.to_gpu()\n",
    "\n",
    "optimizer_1 = chainer.optimizers.MomentumSGD(lr=1.0, momentum=0.9)\n",
    "optimizer_1.setup(model_1)\n",
    "optimizer_1.add_hook(GradientClipping(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，GPUに対応した行列演算モジュールのcupyを呼び出し，学習およびテストデータをcupyの形式に変換します．\n",
    "cupyはnumpyと互換性があります．\n",
    "\n",
    "先ほど読み込んだPTBデータセットを学習に使用するために，データを整理します．\n",
    "まず`bproplen`でネットワークへ入力するデータの長さを指定します．\n",
    "その後，学習データを指定した長さに区切ることで，学習サンプルを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.cupy\n",
    "\n",
    "bproplen = 35\n",
    "\n",
    "train_x, train_y = [], []\n",
    "for idx_window in range(0, len(train) - bproplen - 1, 10):\n",
    "    train_x.append(train[idx_window:idx_window + bproplen])\n",
    "    train_y.append(train[idx_window + 1:idx_window + bproplen + 1])\n",
    "train_x = xp.array(train_x, dtype=xp.int32)\n",
    "train_y = xp.array(train_y, dtype=xp.int32)\n",
    "\n",
    "val = xp.array(val, dtype=xp.int32)\n",
    "test = xp.array(test, dtype=xp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習（教師強制; Teacher forcing）\n",
    "\n",
    "教師強制の方法でネットワークを学習します．\n",
    "教師強制ではネットワークへの入力データとして，教師データ（正しい英単語）を順番に入力し，出力と教師ラベルとの誤差を用いて学習する方法です．\n",
    "\n",
    "１回の誤差を算出するデータ数（ミニバッチサイズ）128，学習エポック数を100とします．\n",
    "先ほど作成したの学習データサイズを取得し，1エポック内における更新回数を求めます．\n",
    "学習データは毎エポックでランダムに利用するため，numpyの`permutation`という関数を利用します．\n",
    "各更新において，学習用データと教師データをそれぞれ`x`と`t`とし，`to_gpu`関数でGPUに転送します．\n",
    "学習モデルにxを与えて各クラスの確率`y`を取得します．\n",
    "各クラスの確率`y`と教師ラベル`t`との誤差を`softmax_coross_entropy`誤差関数で算出します．\n",
    "そして，誤差を`backward`関数で逆伝播し，ネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, mean loss: 204.83774, elapsed_time: 134.35466313362122\n",
      "epoch: 2, mean loss: 179.3819, elapsed_time: 266.44294118881226\n",
      "epoch: 3, mean loss: 174.54474, elapsed_time: 399.2211682796478\n",
      "epoch: 4, mean loss: 171.33682, elapsed_time: 531.7820127010345\n",
      "epoch: 5, mean loss: 168.87186, elapsed_time: 663.3989844322205\n",
      "epoch: 6, mean loss: 167.00325, elapsed_time: 794.8154542446136\n",
      "epoch: 7, mean loss: 165.25616, elapsed_time: 926.416939496994\n",
      "epoch: 8, mean loss: 164.07014, elapsed_time: 1057.943927526474\n",
      "epoch: 9, mean loss: 163.02634, elapsed_time: 1189.2880654335022\n",
      "epoch: 10, mean loss: 162.14458, elapsed_time: 1320.9084856510162\n",
      "epoch: 11, mean loss: 161.40831, elapsed_time: 1452.06933927536\n",
      "epoch: 12, mean loss: 160.66582, elapsed_time: 1583.1417355537415\n",
      "epoch: 13, mean loss: 160.08777, elapsed_time: 1714.47500705719\n",
      "epoch: 14, mean loss: 159.61974, elapsed_time: 1845.87824344635\n",
      "epoch: 15, mean loss: 159.10588, elapsed_time: 1977.3560981750488\n",
      "epoch: 16, mean loss: 158.79295, elapsed_time: 2108.6109862327576\n",
      "epoch: 17, mean loss: 158.49635, elapsed_time: 2239.7740981578827\n",
      "epoch: 18, mean loss: 158.1539, elapsed_time: 2370.9289655685425\n",
      "epoch: 19, mean loss: 157.91129, elapsed_time: 2502.092287540436\n",
      "epoch: 20, mean loss: 157.73135, elapsed_time: 2633.32537150383\n",
      "epoch: 21, mean loss: 157.39915, elapsed_time: 2764.467065811157\n",
      "epoch: 22, mean loss: 157.26053, elapsed_time: 2895.578517436981\n",
      "epoch: 23, mean loss: 157.07979, elapsed_time: 3026.865151166916\n",
      "epoch: 24, mean loss: 156.91136, elapsed_time: 3157.938361644745\n",
      "epoch: 25, mean loss: 156.74356, elapsed_time: 3288.8820464611053\n",
      "epoch: 26, mean loss: 156.51822, elapsed_time: 3420.047792196274\n",
      "epoch: 27, mean loss: 156.45155, elapsed_time: 3551.097817659378\n",
      "epoch: 28, mean loss: 156.37567, elapsed_time: 3682.075731754303\n",
      "epoch: 29, mean loss: 156.24417, elapsed_time: 3813.3695487976074\n",
      "epoch: 30, mean loss: 156.2142, elapsed_time: 3944.6604874134064\n",
      "epoch: 31, mean loss: 155.94066, elapsed_time: 4075.8984587192535\n",
      "epoch: 32, mean loss: 155.89278, elapsed_time: 4207.1305384635925\n",
      "epoch: 33, mean loss: 155.78961, elapsed_time: 4338.328812122345\n",
      "epoch: 34, mean loss: 155.57652, elapsed_time: 4469.453594446182\n",
      "epoch: 35, mean loss: 155.58832, elapsed_time: 4600.76744222641\n",
      "epoch: 36, mean loss: 155.4913, elapsed_time: 4732.005638837814\n",
      "epoch: 37, mean loss: 155.30153, elapsed_time: 4863.350580215454\n",
      "epoch: 38, mean loss: 155.27362, elapsed_time: 4995.109563112259\n",
      "epoch: 39, mean loss: 155.19351, elapsed_time: 5126.511750459671\n",
      "epoch: 40, mean loss: 155.17126, elapsed_time: 5257.985458850861\n",
      "epoch: 41, mean loss: 155.15636, elapsed_time: 5389.2580580711365\n",
      "epoch: 42, mean loss: 155.08429, elapsed_time: 5520.680186510086\n",
      "epoch: 43, mean loss: 155.02933, elapsed_time: 5651.997823476791\n",
      "epoch: 44, mean loss: 154.9427, elapsed_time: 5783.565569400787\n",
      "epoch: 45, mean loss: 154.82864, elapsed_time: 5914.651582956314\n",
      "epoch: 46, mean loss: 154.73453, elapsed_time: 6046.167428255081\n",
      "epoch: 47, mean loss: 154.7567, elapsed_time: 6177.48713684082\n",
      "epoch: 48, mean loss: 154.70282, elapsed_time: 6308.996462583542\n",
      "epoch: 49, mean loss: 154.61272, elapsed_time: 6440.508528709412\n",
      "epoch: 50, mean loss: 154.56331, elapsed_time: 6572.168668985367\n",
      "epoch: 51, mean loss: 154.60721, elapsed_time: 6703.519494533539\n",
      "epoch: 52, mean loss: 154.52307, elapsed_time: 6834.9333465099335\n",
      "epoch: 53, mean loss: 154.56175, elapsed_time: 6966.279834747314\n",
      "epoch: 54, mean loss: 154.3938, elapsed_time: 7097.59890794754\n",
      "epoch: 55, mean loss: 154.30804, elapsed_time: 7229.061325073242\n",
      "epoch: 56, mean loss: 154.25734, elapsed_time: 7360.699791669846\n",
      "epoch: 57, mean loss: 154.17255, elapsed_time: 7492.1300275325775\n",
      "epoch: 58, mean loss: 154.23381, elapsed_time: 7623.8167724609375\n",
      "epoch: 59, mean loss: 154.10162, elapsed_time: 7755.199028015137\n",
      "epoch: 60, mean loss: 154.04964, elapsed_time: 7886.834688901901\n",
      "epoch: 61, mean loss: 154.07307, elapsed_time: 8018.6166706085205\n",
      "epoch: 62, mean loss: 154.04068, elapsed_time: 8149.751553297043\n",
      "epoch: 63, mean loss: 154.06256, elapsed_time: 8280.819064378738\n",
      "epoch: 64, mean loss: 154.02266, elapsed_time: 8412.312980890274\n",
      "epoch: 65, mean loss: 153.92058, elapsed_time: 8543.68187379837\n",
      "epoch: 66, mean loss: 154.07533, elapsed_time: 8675.133743047714\n",
      "epoch: 67, mean loss: 154.02783, elapsed_time: 8806.425891399384\n",
      "epoch: 68, mean loss: 153.89955, elapsed_time: 8937.561660289764\n",
      "epoch: 69, mean loss: 153.9565, elapsed_time: 9068.367287158966\n",
      "epoch: 70, mean loss: 153.89807, elapsed_time: 9199.91252565384\n",
      "epoch: 71, mean loss: 153.8986, elapsed_time: 9331.206554174423\n",
      "epoch: 72, mean loss: 153.86353, elapsed_time: 9462.687422990799\n",
      "epoch: 73, mean loss: 153.81934, elapsed_time: 9593.978077173233\n",
      "epoch: 74, mean loss: 153.74315, elapsed_time: 9725.482696771622\n",
      "epoch: 75, mean loss: 153.62486, elapsed_time: 9856.36511182785\n",
      "epoch: 76, mean loss: 153.74385, elapsed_time: 9988.096332788467\n",
      "epoch: 77, mean loss: 153.74374, elapsed_time: 10120.086316347122\n",
      "epoch: 78, mean loss: 153.76979, elapsed_time: 10251.758579969406\n",
      "epoch: 79, mean loss: 153.68251, elapsed_time: 10382.50990319252\n",
      "epoch: 80, mean loss: 153.67595, elapsed_time: 10513.168843269348\n",
      "epoch: 81, mean loss: 153.7077, elapsed_time: 10643.747007846832\n",
      "epoch: 82, mean loss: 153.63162, elapsed_time: 10774.527746200562\n",
      "epoch: 83, mean loss: 153.66678, elapsed_time: 10905.260785341263\n",
      "epoch: 84, mean loss: 153.63052, elapsed_time: 11036.147359132767\n",
      "epoch: 85, mean loss: 153.71391, elapsed_time: 11166.835482120514\n",
      "epoch: 86, mean loss: 153.61275, elapsed_time: 11297.505383729935\n",
      "epoch: 87, mean loss: 153.63034, elapsed_time: 11428.072618961334\n",
      "epoch: 88, mean loss: 153.54489, elapsed_time: 11558.665292739868\n",
      "epoch: 89, mean loss: 153.57797, elapsed_time: 11689.326960802078\n",
      "epoch: 90, mean loss: 153.52472, elapsed_time: 11820.036857366562\n",
      "epoch: 91, mean loss: 153.47122, elapsed_time: 11950.66364812851\n",
      "epoch: 92, mean loss: 153.48457, elapsed_time: 12081.156740427017\n",
      "epoch: 93, mean loss: 153.352, elapsed_time: 12212.116234779358\n",
      "epoch: 94, mean loss: 153.42668, elapsed_time: 12342.735532045364\n",
      "epoch: 95, mean loss: 153.42645, elapsed_time: 12473.377804756165\n",
      "epoch: 96, mean loss: 153.39595, elapsed_time: 12604.24705028534\n",
      "epoch: 97, mean loss: 153.28444, elapsed_time: 12734.950905799866\n",
      "epoch: 98, mean loss: 153.32921, elapsed_time: 12865.752504825592\n",
      "epoch: 99, mean loss: 153.2774, elapsed_time: 12996.593631029129\n",
      "epoch: 100, mean loss: 153.37476, elapsed_time: 13127.348662376404\n"
     ]
    }
   ],
   "source": [
    "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
    "batch_size = 128\n",
    "epoch_num = 100\n",
    "train_data_num = train_x.shape[0]\n",
    "num_iter_per_epoch = int(train_data_num / batch_size)\n",
    "\n",
    "\n",
    "# 学習の実行\n",
    "start = time()\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    perm = xp.random.permutation(train_data_num)\n",
    "    \n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        \n",
    "        x = Variable(cuda.to_gpu(train_x[perm[i:i+batch_size]]))\n",
    "        t = Variable(cuda.to_gpu(train_y[perm[i:i+batch_size]]))\n",
    "        \n",
    "        accum_loss = 0\n",
    "        model_1.reset_state()\n",
    "        \n",
    "        for idx_window in range(bproplen):\n",
    "            \n",
    "            y = model_1(x[:, idx_window])\n",
    "            \n",
    "            loss = F.softmax_cross_entropy(y, t[:, idx_window])\n",
    "            accum_loss += loss\n",
    "            sum_loss += loss.data\n",
    "\n",
    "        optimizer_1.target.cleargrads()\n",
    "        accum_loss.backward()\n",
    "        accum_loss.unchain_backward()\n",
    "        optimizer_1.update()\n",
    "\n",
    "    elapsed_time = time() - start\n",
    "    print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch,\n",
    "                                                              sum_loss/num_iter_per_epoch,\n",
    "                                                              elapsed_time))\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        model.to_cpu()\n",
    "        chainer.serializers.save_npz(\"rnn-%03d.npz\" % epoch, model)\n",
    "        model.to_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習（教師強制を用いない学習）\n",
    "\n",
    "こちらでは，教師強制を用いない学習を行います．\n",
    "この方法は，ネットワークから出力された結果（予測された英単語）を次の入力として順番に使用し学習する方法です．\n",
    "\n",
    "\n",
    "まず，先ほどとは異なるネットワークとして`model_2`を作成し，最適化手法を設定します．\n",
    "この時のパラメータは同じものを使用します．\n",
    "\n",
    "最初の入力のみ`x`すなわち正しい単語を入力し，それ以外の入力には前の時刻の結果`y`から求められた単語を入力します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの作成\n",
    "model_2 = RNNLM(n_vocab=num_vocab, n_units=num_units)\n",
    "model_2.to_gpu()\n",
    "\n",
    "# 最適化手法の設定\n",
    "optimizer_2 = chainer.optimizers.MomentumSGD(lr=1.0, momentum=0.9)\n",
    "optimizer_2.setup(model_2)\n",
    "optimizer_2.add_hook(GradientClipping(5.0))\n",
    "\n",
    "\n",
    "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
    "batch_size = 128\n",
    "epoch_num = 100\n",
    "train_data_num = train_x.shape[0]\n",
    "num_iter_per_epoch = int(train_data_num / batch_size)\n",
    "\n",
    "\n",
    "# 学習の実行\n",
    "start = time()\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    perm = xp.random.permutation(train_data_num)\n",
    "    \n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "        \n",
    "        x = Variable(cuda.to_gpu(train_x[perm[i:i+batch_size]]))\n",
    "        t = Variable(cuda.to_gpu(train_y[perm[i:i+batch_size]]))\n",
    "\n",
    "        pred = None\n",
    "        accum_loss = 0\n",
    "        model_2.reset_state()\n",
    "\n",
    "        for idx_window in range(bproplen):\n",
    "            \n",
    "            if idx_window == 0:\n",
    "                y = model_2(x[:, idx_window])\n",
    "            else:\n",
    "                y = model_2(pred)\n",
    "                    \n",
    "            pred = F.argmax(y, axis=1)\n",
    "            \n",
    "            loss = F.softmax_cross_entropy(y, t[:, idx_window])\n",
    "            accum_loss += loss\n",
    "            sum_loss += loss.data\n",
    "            \n",
    "        optimizer_2.target.cleargrads()\n",
    "        accum_loss.backward()\n",
    "        accum_loss.unchain_backward()\n",
    "        optimizer_2.update()\n",
    "            \n",
    "    elapsed_time = time() - start\n",
    "    print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch,\n",
    "                                                              sum_loss/num_iter_per_epoch,\n",
    "                                                              elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "学習したネットワークモデルを用いて評価を行います．\n",
    "\n",
    "\n",
    "### 1. 教師強制ありのモデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentense = []\n",
    "pred_sentense = []\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    pred_word = None\n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_1(x)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "    \n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_1(pred)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "\n",
    "print(' '.join(true_sentense[0:10]))\n",
    "print(' '.join(pred_sentense[9:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 教師強制なしのモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentense = []\n",
    "pred_sentense = []\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    pred_word = None\n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_2(x)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "    \n",
    "    for i in range(10):\n",
    "        x = Variable(cuda.to_gpu(test[i].reshape(1, 1)))\n",
    "        y = model_2(pred)\n",
    "        pred = F.argmax(y, axis=1)\n",
    "        \n",
    "        true_word = inverse_vocab[int(cuda.to_cpu(x.data[0]))]\n",
    "        pred_word = inverse_vocab[int(cuda.to_cpu(pred.data[0]))]\n",
    "        true_sentense.append(true_word)\n",
    "        pred_sentense.append(pred_word)\n",
    "\n",
    "print(' '.join(true_sentense[0:10]))\n",
    "print(' '.join(pred_sentense[9:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "1. ネットワークのLSTM層の数を変更した際の性能変化を確認しましょう\n",
    "2. ネットワークのLSTMをGRUに変更して性能の変化を確認しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
