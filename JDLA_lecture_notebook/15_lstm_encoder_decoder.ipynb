{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoderによる機械翻訳\n",
    "\n",
    "---\n",
    "## 目的\n",
    "Encoder-Decoder構造のネットワークである（Sequence-to-Sequenel Seq2Seq）を用いて機械翻訳（English-French）を行う．\n",
    "\n",
    "\n",
    "## 対応するチャプター\n",
    "* 10.4: Encoder-DecoderとSequence-to-Sequence\n",
    "\n",
    "## データのダウンロード\n",
    "実習に必要なデータをダウンロードします．\n",
    "下記のコードを実行してデータのダウンロードを行ってください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.mprg.cs.chubu.ac.jp/~hirakawa/share/tutorial_data/fra-eng-tr_data.zip\n",
    "!unzip -q -o fra-eng-tr_data.zip\n",
    "!ls ./fra-eng-tr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import json\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import get_ptb_words, get_ptb_words_vocabulary\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.optimizer_hooks import GradientClipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "データセットを読み込みます．\n",
    "今回のデータはフランス語と英語の対訳データセットを使用します．\n",
    "使用するデータには，学習データとして8479，テストデータとして2120の対訳文が含まれています．\n",
    "\n",
    "また，このデータセットに存在する単語の情報を取得します．\n",
    "`fra_vocab`にはフランス語の単語情報，`eng_vocab`には英語の単語情報を含んだ辞書オブジェクトが読み込まれます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_text_dataset(input_filename):\n",
    "    downloaded = []\n",
    "    with open(input_filename) as f:\n",
    "        for s in f.readlines():\n",
    "            downloaded.append( np.array(list(map(int, s.strip().split(' '))), dtype=np.int32) )\n",
    "    return downloaded\n",
    "\n",
    "fra_train = download_text_dataset(\"fra-eng-tr_data/fra_train.txt\")\n",
    "eng_train = download_text_dataset(\"fra-eng-tr_data/eng_train.txt\")\n",
    "fra_test = download_text_dataset(\"fra-eng-tr_data/fra_test.txt\")\n",
    "eng_test = download_text_dataset(\"fra-eng-tr_data/eng_test.txt\")\n",
    "\n",
    "print(len(fra_train))\n",
    "print(len(fra_test))\n",
    "        \n",
    "with open(\"fra-eng-tr_data/fra_vocab.json\") as f:\n",
    "    fra_vocab = json.load(f)\n",
    "    fra_vocab = {int(k):v for k, v in fra_vocab.items()}\n",
    "    \n",
    "with open(\"fra-eng-tr_data/eng_vocab.json\") as f:\n",
    "    eng_vocab = json.load(f)\n",
    "    eng_vocab = {int(k):v for k, v in eng_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "Seq2Seqのネットワークを定義します．\n",
    "\n",
    "文章の終わりを表現する単語IDとして`EOS=1`を定義します．\n",
    "\n",
    "次に，`sequence_embed`関数を定義します．\n",
    "この関数では，この次に定義するネットワーク内部での計算を行う際に，任意の長さの文章をまとめたミニバッチデータに対して，ある処理を行うための関数です．\n",
    "この関数を用いてネットワークの演算を行うことで，ミニバッチ中に含まれる文章の長さが異なる場合でも，一度に処理を行うことが可能となります．\n",
    "\n",
    "翻訳を行うための`Seq2seq`ネットワークを定義します．\n",
    "このネットワークには，入力された文章（フランス語文）を処理するEncoderと翻訳結果（英文）を出力するDecoderが存在します．\n",
    "\n",
    "まず`__init__`関数でネットワーク内部の層を定義します．\n",
    "`__init__`関数の引数として，次の引数を準備します．\n",
    "`n_layers`はEncoderおよびDecoderに用いるLSTMの総数，`n_source_vocab`は入力側で扱う単語数，`n_target_vocab`は出力側で扱う単語数，`n_units`はLSTMの隠れ層のユニット数です．\n",
    "`embed_x`および`embed_y`ではそれぞれ，入力・出力側で扱う単語のIDから単語を表現した特徴ベクトルを出力するためのembed層を定義します．\n",
    "次に，`encoder`および`decoder`では，LSTM層を定義します．\n",
    "ここでは，`NStepLSTM`という層を使用して定義を行います．\n",
    "`NStepLSTM`は，任意の長さのデータ（文章）をまとめたミニバッチを同時に扱うことができるLSTMの定義方法です．\n",
    "ここでは，LSTMの総数やdropoutのdrop率も同時に定義することが可能です．\n",
    "最後に，`decoder`から出力された特徴を元に翻訳結果の単語IDを出力するための出力層`W`を定義します．\n",
    "\n",
    "次に，入力されたデータから結果を出力するための演算を定義します．\n",
    "ここでは，下記で実行する学習のプログラムを簡単にするため，2種類の演算の関数を定義します．\n",
    "`__call__`関数では，学習を行う際の演算を定義します．ネットワークの出力結果と正解の翻訳情報から計算した誤差を返す関数として定義します．\n",
    "また，`translate`関数では，ネットワークが予測した翻訳結果を出力するように定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = 1\n",
    "\n",
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = np.cumsum(x_len[:-1])\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs\n",
    "\n",
    "class Seq2seq(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed_x = L.EmbedID(n_source_vocab, n_units)\n",
    "            self.embed_y = L.EmbedID(n_target_vocab, n_units)\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.decoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.W = L.Linear(n_units, n_target_vocab)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        xs = [x[::-1] for x in xs]\n",
    "\n",
    "        eos = self.xp.array([EOS], np.int32)\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n",
    "\n",
    "        # Both xs and ys_in are lists of arrays.\n",
    "        exs = sequence_embed(self.embed_x, xs)\n",
    "        eys = sequence_embed(self.embed_y, ys_in)\n",
    "\n",
    "        batch = len(xs)\n",
    "        # None represents a zero vector in an encoder.\n",
    "        hx, cx, _ = self.encoder(None, None, exs)\n",
    "        _, _, os = self.decoder(hx, cx, eys)\n",
    "\n",
    "        # It is faster to concatenate data before calculating loss\n",
    "        # because only one matrix multiplication is called.\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch\n",
    "\n",
    "        n_words = concat_ys_out.shape[0]\n",
    "        perp = self.xp.exp(loss.array * batch / n_words)\n",
    "        return loss\n",
    "\n",
    "    def translate(self, xs, max_length=100):\n",
    "        batch = len(xs)\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            xs = [x[::-1] for x in xs]\n",
    "            exs = sequence_embed(self.embed_x, xs)\n",
    "            h, c, _ = self.encoder(None, None, exs)\n",
    "            ys = self.xp.full(batch, EOS, np.int32)\n",
    "            result = []\n",
    "            for i in range(max_length):\n",
    "                eys = self.embed_y(ys)\n",
    "                eys = F.split_axis(eys, batch, 0)\n",
    "                h, c, ys = self.decoder(h, c, eys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.array, axis=1).astype(np.int32)\n",
    "                result.append(ys)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Dwuvfouzmd7"
   },
   "source": [
    "## ネットワークの作成\n",
    "上のプログラムで定義したネットワークを作成します．\n",
    "ここでは，GPUで学習を行うために，modelをGPUに送るto_gpu関数を利用しています．\n",
    "\n",
    "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．また，学習率を0.001として引数に与えます．そして，最適化方法のsetup関数にネットワークモデルを与えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_source_vocab = len(fra_vocab)\n",
    "num_target_vocab = len(eng_vocab)\n",
    "model = Seq2seq(n_layers=2,\n",
    "                n_source_vocab=num_source_vocab,\n",
    "                n_target_vocab=num_target_vocab,\n",
    "                n_units=1024)\n",
    "model.to_gpu()\n",
    "\n",
    "optimizer = chainer.optimizers.MomentumSGD(lr=0.001, momentum=0.9)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(GradientClipping(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "学習を実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.cupy\n",
    "\n",
    "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
    "batch_size = 32\n",
    "epoch_num = 100\n",
    "train_data_num = len(fra_train)\n",
    "num_iter_per_epoch = int(train_data_num / batch_size)\n",
    "\n",
    "# 学習の実行\n",
    "start = time()\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    perm = np.random.permutation(train_data_num)\n",
    "    for i in range(0, train_data_num, batch_size):\n",
    "\n",
    "        fra_batch = [ Variable(cuda.to_gpu(np.array(fra_train[ii], dtype=np.int32))) for ii in perm[i:i+batch_size] ]\n",
    "        eng_batch = [ Variable(cuda.to_gpu(np.array(eng_train[ii], dtype=np.int32))) for ii in perm[i:i+batch_size] ]\n",
    "        \n",
    "        loss = model(fra_batch, eng_batch)\n",
    "        \n",
    "        sum_loss += loss.data\n",
    "\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "    elapsed_time = time() - start\n",
    "    print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch,\n",
    "                                                              sum_loss/num_iter_per_epoch,\n",
    "                                                              elapsed_time))\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        model.to_cpu()\n",
    "        chainer.serializers.save_npz(\"model-%03d.npz\" % epoch, model)\n",
    "        model.to_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "\n",
    "学習後のネットワークを用いて，翻訳を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    test_index = np.arange(20)\n",
    "    \n",
    "    for i_test in test_index:\n",
    "        fra_batch = [ Variable(cuda.to_gpu(np.array(fra_train[i_test], dtype=np.int32)))]\n",
    "        eng_batch = [ np.array(eng_train[i_test], dtype=np.int32)]\n",
    "    \n",
    "        y = model.translate(fra_batch, max_length=10)\n",
    "    \n",
    "        fra_true = [fra_vocab[i] for i in cuda.to_cpu(fra_batch[0].data)]\n",
    "        eng_pred = [eng_vocab[cuda.to_cpu(i)[0]] for i in y]\n",
    "\n",
    "        print(\"input french sentence      :\", \" \".join(fra_true))\n",
    "        print(\"translated english sentence:\", \" \".join(eng_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト（学習済みモデル）\n",
    "\n",
    "Seq2Seqの学習には時間を要するため，下記のコードでは学習済みのモデルを読み込んで，学習後のネットワークでの翻訳結果を確認します．\n",
    "保存したモデルパラメータを読み込んで，テストデータの翻訳を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルの読み込み\n",
    "num_source_vocab = len(fra_vocab)\n",
    "num_target_vocab = len(eng_vocab)\n",
    "pretrained_model = Seq2seq(n_layers=2,\n",
    "                n_source_vocab=num_source_vocab,\n",
    "                n_target_vocab=num_target_vocab,\n",
    "                n_units=1024)\n",
    "\n",
    "chainer.serializers.load_npz(\"fra-eng-tr_data/seq2seq.npz\", pretrained_model)\n",
    "pretrained_model.to_gpu()\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    test_index = np.arange(20)\n",
    "    \n",
    "    for i_test in test_index:\n",
    "        fra_batch = [ Variable(cuda.to_gpu(np.array(fra_train[i_test], dtype=np.int32)))]\n",
    "        eng_batch = [ np.array(eng_train[i_test], dtype=np.int32)]\n",
    "    \n",
    "        y = pretrained_model.translate(fra_batch, max_length=15)\n",
    "    \n",
    "        fra_true = [fra_vocab[i] for i in cuda.to_cpu(fra_batch[0].data)]\n",
    "        eng_pred = [eng_vocab[cuda.to_cpu(i)[0]] for i in y]\n",
    "\n",
    "        print(\"input french sentence      :\", \" \".join(fra_true))\n",
    "        print(\"translated english sentence:\", \" \".join(eng_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
