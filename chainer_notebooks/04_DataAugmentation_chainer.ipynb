{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCWsTzQXtZI7"
   },
   "source": [
    "# Data AugmentationによるCNNの精度向上\n",
    "\n",
    "##本チュートリアルではchainerを利用してニューラルネットワークの実装を確認，学習および評価を行います．　環境としてはGoogle が提供する Google Colaboratory上でおこないます． GPU上で処理を行うため，colaboratoryの[ランタイム]->[ランタイムのタイプを変更]からハードウェアアクセラレータをGPUにしてください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30MMRPy8tzWt"
   },
   "source": [
    "Goolge Colaboratory上にChainerとCupyをインストールします．Data augmentationには，ChainerCVで用意されているクラスを利用するので，ChainerCVのインストールも行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynASwq7YOwFZ"
   },
   "outputs": [],
   "source": [
    "!curl https://colab.chainer.org/install | sh -\n",
    "!pip install  chainercv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Esf4xv7uoEr"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Chainerでニューラルネットワークを学習するために必要なモジュールや関数をインポートします．また，data augmentationを行うために，画像変換にopencvを利用します．opencvのモジュールであるcv2も合わせてインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qolZekhukSL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda,optimizers, serializers, Chain, Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainercv import transforms\n",
    "from functools import partial\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORwBLLmjwhF0"
   },
   "source": [
    "GPUが利用できるか確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ItMpSpnwmHU"
   },
   "outputs": [],
   "source": [
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kYv__Gv8DgC"
   },
   "source": [
    "次に学習データを読み込みます．CIFAR10データセットは中規模な一般物体認識のデータセットであり，chainerでは CIFAR10\n",
    "データセットを取得し，学習するためのフォーマットに変換してくれます．データセットには学習用とテスト用のデータに別れており，それぞれtrain_dataset, test_datasetとします．　学習データについて，平均と標準偏差を求めておきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCv8tqrY76Ug"
   },
   "outputs": [],
   "source": [
    "#train_dataset, test_dataset = chainer.datasets.get_cifar10(scale=255.)\n",
    "train_dataset, test_dataset = chainer.datasets.get_cifar10()\n",
    "mean = np.mean([x for x, _ in train_dataset], axis=(0, 2, 3))\n",
    "std = np.std([x for x, _ in train_dataset], axis=(0, 2, 3))\n",
    "print (len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRrkVN6cdyr_"
   },
   "source": [
    "Data augmentationを行う関数をtransformとして定義します．この関数は学習時のみ利用します．画像を左右反転させるrandom_flip関数，画像を拡大縮小するrandom_expand関数，画像を一定サイズに切り出すrandom_crop関数を利用します．これらの関数はChainerCVのtransformクラスに定義されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRHjjCUVdxfd"
   },
   "outputs": [],
   "source": [
    "def transform(  inputs, mean, std, expand_ratio=1.0, crop_size=(32, 32), train=True):\n",
    "    img, label = inputs\n",
    "    img = img.copy()\n",
    "\n",
    "    # Standardization\n",
    "#    img -= mean[:, None, None]\n",
    "#    img /= std[:, None, None]\n",
    "\n",
    "    if train:\n",
    "        # Random flip\n",
    "        img = transforms.random_flip(img, x_random=True)\n",
    "        # Random expand\n",
    "        if expand_ratio > 1:\n",
    "            img = transforms.random_expand(img, max_ratio=expand_ratio)\n",
    "        # Random crop\n",
    "        if tuple(crop_size) != (32, 32):\n",
    "            img = transforms.random_crop(img, tuple(crop_size))\n",
    "\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1pzZryyenRk"
   },
   "source": [
    "先ほど定義したtransform関数を学習用のtrain_transform，評価用のvalid_transformとします．学習用のtrain_transformには拡大縮小の比率を1.2倍，切り出しサイズを28x28とするように引数で指定します．TransformDatasetクラスは引数に与えたデータセットを同じく引数に与えたdata augmentationの関数に与えて変形させていきます．変形処理はtrain_datasetを呼び出すタイミングで逐次行われます． Partial関数は，第一引数の関数を第二引数以降の引数を与えて随時呼び出す高階関数です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qQoFXAvenrH"
   },
   "outputs": [],
   "source": [
    "train_transform = partial( transform, mean=mean, std=std, expand_ratio=1.2, crop_size=[28, 28], train=True)\n",
    "valid_transform = partial(transform, mean=mean, std=std, train=False)\n",
    "\n",
    "train_dataset = TransformDataset(train_dataset, train_transform)\n",
    "test_dataset = TransformDataset(test_dataset, valid_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o05arGVVu8Uq"
   },
   "source": [
    "畳み込みニューラルネットワークを定義します．ここでは，畳み込み層２層，全結合層３層から構成されるネットワークとします．１層目の畳み込み層は入力チャンネル数が１，出力する特徴マップ数が16，畳み込むフィルタサイズが3x3です．２層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が32，畳み込むフィルタサイズは同じく3x3です．１つ目の全結合層は入力ユニット数は不定とし，出力は1024としています．次の全結合層入力，出力共に1024，出力層は入力が1024，出力が10です．これらの各層の構成を\\__init\\__関数で定義します．\n",
    "次に，\\__call\\__関数では，定義した層を接続して処理するように記述します．\\__call\\__関数の引数xは入力データです．それを\\__init\\__関数で定義したconv1に与え，その出力を活性化関数であるrelu関数に与えます．そして，その出力をmax_pooling_2dに与えて，プーリング処理結果をhとして出力します．hはconv2に与えられて畳み込み処理とプーリング処理を行います．そして，出力hをl1に与えて全結合層の処理を行います．最終的にl3の全結合層の処理を行った出力hを戻り値としています．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4hAwTlKuzVT"
   },
   "outputs": [],
   "source": [
    "class CNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__(\n",
    "            conv1 = L.Convolution2D(3, 16, 3, pad=1), \n",
    "            conv2 = L.Convolution2D(16, 32, 3, pad=1), \n",
    "            l1 = L.Linear(None, 1024),\n",
    "            l2 = L.Linear(1024, 1024),\n",
    "            l3 = L.Linear(1024, 10)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        h = F.max_pooling_2d(F.relu(self.conv1(x)), 2)\n",
    "        h = F.max_pooling_2d(F.relu(self.conv2(h)), 2)\n",
    "        h = F.relu(self.l1(h))\n",
    "        h = F.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hYTV75a79OS"
   },
   "source": [
    "畳み込みネットワークモデルを定義します．学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．また，学習率を0.05として引数に与えます．そして，最適化方法のsetup関数にネットワークモデルを与えます．ここでは，GPUで学習を行うために，modelをGPUに送るto_gpu関数を利用しています．また，GPUに対応した行列演算モジュールのcupyを呼び出しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rdmJMVY75vq"
   },
   "outputs": [],
   "source": [
    "gpu_id = 0 \n",
    "xp = cuda.cupy\n",
    "model = CNN()\n",
    "model.to_gpu(gpu_id)\n",
    "\n",
    "learnrate = 0.05\n",
    "optimizer = chainer.optimizers.MomentumSGD(learnrate)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUNa9Xe79vAG"
   },
   "source": [
    "１回の誤差を算出するデータ数（ミニバッチサイズ）を32，学習エポック数を100とします．CIFAR10の学習データサイズを取得し，１エポック内における更新回数を求めます．まず，各エポックにおいて学習データのaugmentationを行います．学習率は25エポックごとに0.5倍して徐々に小さくしていくようにします．学習データは毎エポックでランダムに利用するため，numpyのpermutationという関数を利用します．各更新において，学習用データと教師データをそれぞれxとtとし，to_gpu関数でGPUに転送します．学習モデルにxを与えて各クラスの確率yを取得します．各クラスの確率yと教師ラベルtとの誤差をsoftmax coross entropy誤差関数で算出します．また，認識精度も算出します．そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68RE3RTa76-W"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epoch_num = 100\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epoch_num):\n",
    "        dataset_x = []\n",
    "        dataset_y =[]\n",
    "        for train in train_dataset:\n",
    "            dataset_x.append(train[0])\n",
    "            dataset_y.append(train[1])\n",
    "  \n",
    "        train_x = xp.asarray(dataset_x, xp.float32)\n",
    "        train_y = xp.asarray(dataset_y, xp.int32)\n",
    "\n",
    "        train_data_num = train_x.shape[0]\n",
    "        iter_one_epoch = int(train_x.shape[0]/batch_size)\n",
    "  \n",
    "        num_iter = 0\n",
    "        sum_loss = 0\n",
    "        sum_accuracy = 0\n",
    "        if (epoch+1) % 25 == 0 :\n",
    "            optimizer.lr *= 0.5\n",
    "        perm = xp.random.permutation(train_data_num)\n",
    "        for i in range(0, train_data_num, batch_size):\n",
    "                x = Variable(cuda.to_gpu(train_x[perm[i:i+batch_size]]))\n",
    "                t = Variable(cuda.to_gpu(train_y[perm[i:i+batch_size]]))\n",
    "                y = model(x)        \n",
    "                model.zerograds()\n",
    "                loss = F.softmax_cross_entropy(y, t)\n",
    "                acc = F.accuracy(y, t)\n",
    "                loss.backward()\n",
    "                optimizer.update()\n",
    "                sum_loss += loss.data\n",
    "                sum_accuracy += acc.data\n",
    "                num_iter +=1\n",
    "        elapsed_time = time.time() - start\n",
    "        print(\"epoch: {}, mean loss: {}, mean accuracy: {},  elapsed_time :{}\".format(epoch+1, sum_loss/num_iter, sum_accuracy/num_iter, elapsed_time))\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            chainer.serializers.save_hdf5('caifr10_epoch{}.npz'.format(epoch), model, compression=6)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6OZY7GgDBPl"
   },
   "source": [
    "学習できたネットワークモデルを利用して評価を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03_8Wh5V9F6V"
   },
   "outputs": [],
   "source": [
    "chainer.config.train=False\n",
    "\n",
    "cnt = 0\n",
    "dataset_x = []\n",
    "dataset_y =[]\n",
    "for test in test_dataset:\n",
    "    dataset_x.append(transforms.resize(test[0], [28,28]))\n",
    "    dataset_y.append(test[1])\n",
    "test_x = xp.asarray(dataset_x, xp.float32)\n",
    "test_y = xp.asarray(dataset_y, xp.int32)\n",
    "print (test_x.shape, test_y.shape)\n",
    "  \n",
    "test_data_num = test_x.shape[0]\n",
    "for i in range(0, test_data_num):\n",
    "       x = Variable(cuda.to_gpu(test_x[i].reshape(1,3,28,28)))\n",
    "       t = test_y[i]\n",
    "       y = model(x)        \n",
    "       y = np.argmax(y.data[0])\n",
    "       if t == y:\n",
    "           cnt += 1\n",
    "print(\"test accuracy: {}\".format(cnt/test_data_num))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkbHdXRnP3Pg"
   },
   "source": [
    "ネットワークを３層の畳み込み層と２層の全結合層から構成されたLeNetベースの構造に変えます．畳み込み層のフィルタサイズは5x5，全結合層のユニット数は4096です．また，畳み込み層と全結合層の間にはSpatial Pyramid Poolingという特殊なプーリング層を用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmnOHR9qPkNp"
   },
   "outputs": [],
   "source": [
    "class LeNet5(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(3, 32, 5, stride=1, pad=2)\n",
    "            self.conv2 = L.Convolution2D(32, 32, 5, stride=1, pad=2)\n",
    "            self.conv3 = L.Convolution2D(32, 64, 5, stride=1, pad=2)\n",
    "            self.fc4 = L.Linear(None, 4096)\n",
    "            self.fc5 = L.Linear(4096, n_class)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.max_pooling_2d(F.relu(self.conv1(x)), 3, stride=2)\n",
    "        h = F.max_pooling_2d(F.relu(self.conv2(h)), 3, stride=2)\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.spatial_pyramid_pooling_2d(h, 3, F.MaxPooling2D)\n",
    "        h = F.dropout(F.relu(self.fc4(h)), ratio=0.5)\n",
    "        h = self.fc5(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsjnI6sflu-E"
   },
   "source": [
    "LeNet5をGPUに送ります．そして，最適化にLeNetを引数として与えます．学習率は0.01とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCncOu7KPq4X"
   },
   "outputs": [],
   "source": [
    "gpu_id = 0 \n",
    "xp = cuda.cupy\n",
    "model = LeNet5()\n",
    "model.to_gpu(gpu_id)\n",
    "\n",
    "learnrate = 0.01\n",
    "optimizer = chainer.optimizers.MomentumSGD(learnrate)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EznthXZvls2L"
   },
   "source": [
    "LeNetによる学習を行います．上記にある学習処理を実行します．（注意：この下のコードではありません．）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WpTwuSEmXYq"
   },
   "source": [
    "次にネットワーク構造をVGGにします．VGGは畳み込み層が13層，全結合層が3層です．畳み込み層のフィルタサイズは3x3です．オリジナルのVGGと異なり，学習が収束しやすくするために各畳み込み層後にBatch Normalizationを適用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1K1Z8P57ltRe"
   },
   "outputs": [],
   "source": [
    "class VGG(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "        super(VGG, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1_1 = L.Convolution2D(None, 64, 3, pad=1)\n",
    "            self.bn1_1 = L.BatchNormalization(64)\n",
    "            self.conv1_2 = L.Convolution2D(64, 64, 3, pad=1)\n",
    "            self.bn1_2 = L.BatchNormalization(64)\n",
    "\n",
    "            self.conv2_1 = L.Convolution2D(64, 128, 3, pad=1)\n",
    "            self.bn2_1 = L.BatchNormalization(128)\n",
    "            self.conv2_2 = L.Convolution2D(128, 128, 3, pad=1)\n",
    "            self.bn2_2 = L.BatchNormalization(128)\n",
    "\n",
    "            self.conv3_1 = L.Convolution2D(128, 256, 3, pad=1)\n",
    "            self.bn3_1 = L.BatchNormalization(256)\n",
    "            self.conv3_2 = L.Convolution2D(256, 256, 3, pad=1)\n",
    "            self.bn3_2 = L.BatchNormalization(256)\n",
    "            self.conv3_3 = L.Convolution2D(256, 256, 3, pad=1)\n",
    "            self.bn3_3 = L.BatchNormalization(256)\n",
    "            self.conv3_4 = L.Convolution2D(256, 256, 3, pad=1)\n",
    "            self.bn3_4 = L.BatchNormalization(256)\n",
    "\n",
    "            self.fc4 = L.Linear(None, 1024)\n",
    "            self.fc5 = L.Linear(1024, 1024)\n",
    "            self.fc6 = L.Linear(1024, n_class)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        h = F.relu(self.bn1_2(self.conv1_2(h)))\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = F.dropout(h, ratio=0.25)\n",
    "\n",
    "        h = F.relu(self.bn2_1(self.conv2_1(h)))\n",
    "        h = F.relu(self.bn2_2(self.conv2_2(h)))\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = F.dropout(h, ratio=0.25)\n",
    "\n",
    "        h = F.relu(self.bn3_1(self.conv3_1(h)))\n",
    "        h = F.relu(self.bn3_2(self.conv3_2(h)))\n",
    "        h = F.relu(self.bn3_3(self.conv3_3(h)))\n",
    "        h = F.relu(self.bn3_4(self.conv3_4(h)))\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = F.dropout(h, ratio=0.25)\n",
    "\n",
    "        h = F.dropout(F.relu(self.fc4(h)), ratio=0.5)\n",
    "        h = F.dropout(F.relu(self.fc5(h)), ratio=0.5)\n",
    "        h = self.fc6(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SNbSt48m1YH"
   },
   "outputs": [],
   "source": [
    "gpu_id = 0 \n",
    "xp = cuda.cupy\n",
    "model = VGG()\n",
    "model.to_gpu(gpu_id)\n",
    "\n",
    "learnrate = 0.01\n",
    "optimizer = chainer.optimizers.MomentumSGD(learnrate)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrSRLVWsExwl"
   },
   "source": [
    "## 課題　\n",
    "###以下の課題に取り組みましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knno-MVZ4Oye"
   },
   "source": [
    "1  Data augmentationのパラメータを変えてみましょう．\n",
    "\n",
    "　partial関数の引数に与えている拡大縮小の倍率を1.2から1.5に変えましょう\n",
    "\n",
    "　次に，transform関数でコメントアウトしている平均と標準偏差を利用した処理を行うように修正しましょう\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oYzfKGU4Idy"
   },
   "source": [
    "2  最適化の方法をAdamに変えて実験しましょう．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyGUUy9p4Nwi"
   },
   "source": [
    "3  学習率を変えましょう．\n",
    "\n",
    "　学習率を0.05から0.01に修正しましょう\n",
    "\n",
    "　次に，学習率の減衰率を0.5から0.1にしましょう\n",
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "04_DataAugmentation_chainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
