{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCWsTzQXtZI7"
   },
   "source": [
    "# ChainerによるResidual Network (ResNet)\n",
    "\n",
    "## 本チュートリアルではchainerを利用してニューラルネットワークの実装を確認，学習および評価を行います．　環境としてはGoogle が提供する Google Colaboratory上でおこないます． GPU上で処理を行うため，colaboratoryの[ランタイム]->[ランタイムのタイプを変更]からハードウェアアクセラレータをGPUにしてください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30MMRPy8tzWt"
   },
   "source": [
    "Goolge Colaboratory上にChainerとCupyをインストールします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkksRbnlh8hV"
   },
   "outputs": [],
   "source": [
    "!curl https://colab.chainer.org/install | sh -\n",
    "!pip install 'chainercv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Esf4xv7uoEr"
   },
   "source": [
    "Chainerでニューラルネットワークを学習するために必要なモジュールや関数をインポートします．また，data augmentationを行うために，画像変換にopencvを利用します．opencvのモジュールであるcv2も合わせてインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qolZekhukSL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda,optimizers, serializers, Chain, Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainercv import transforms\n",
    "from functools import partial\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORwBLLmjwhF0"
   },
   "source": [
    "GPUが利用できるか確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ItMpSpnwmHU"
   },
   "outputs": [],
   "source": [
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o05arGVVu8Uq"
   },
   "source": [
    "畳み込みニューラルネットワークを定義します．ここでは，畳み込み層２層，全結合層３層から構成されるネットワークとします．１層目の畳み込み層は入力チャンネル数が１，出力する特徴マップ数が16，畳み込むフィルタサイズが3x3です．２層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が32，畳み込むフィルタサイズは同じく3x3です．１つ目の全結合層は入力ユニット数は不定とし，出力は1024としています．次の全結合層入力，出力共に1024，出力層は入力が1024，出力が10です．これらの各層の構成を\\__init\\__関数で定義します．\n",
    "次に，\\__call\\__関数では，定義した層を接続して処理するように記述します．\\__call\\__関数の引数xは入力データです．それを\\__init\\__関数で定義したconv1に与え，その出力を活性化関数であるrelu関数に与えます．そして，その出力をmax_pooling_2dに与えて，プーリング処理結果をhとして出力します．hはconv2に与えられて畳み込み処理とプーリング処理を行います．そして，出力hをl1に与えて全結合層の処理を行います．最終的にl3の全結合層の処理を行った出力hを戻り値としています．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4hAwTlKuzVT"
   },
   "outputs": [],
   "source": [
    "class BottleNeck(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out, stride=1, use_conv=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(BottleNeck, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(n_in, n_mid, 1, stride, 0, True, w)\n",
    "            self.bn1 = L.BatchNormalization(n_mid)\n",
    "            self.conv2 = L.Convolution2D(n_mid, n_mid, 3, 1, 1, True, w)\n",
    "            self.bn2 = L.BatchNormalization(n_mid)\n",
    "            self.conv3 = L.Convolution2D(n_mid, n_out, 1, 1, 0, True, w)\n",
    "            self.bn3 = L.BatchNormalization(n_out)\n",
    "            if use_conv:\n",
    "                self.conv4 = L.Convolution2D(\n",
    "                    n_in, n_out, 1, stride, 0, True, w)\n",
    "                self.bn4 = L.BatchNormalization(n_out)\n",
    "        self.use_conv = use_conv\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn1(self.conv1(x)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = self.bn3(self.conv3(h))\n",
    "        return h + self.bn4(self.conv4(x)) if self.use_conv else h + x\n",
    "\n",
    "\n",
    "class Block(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out, n_bottlenecks, stride=2):\n",
    "        super(Block, self).__init__()\n",
    "        self.add_link(BottleNeck(n_in, n_mid, n_out, stride, True))\n",
    "        for _ in range(n_bottlenecks - 1):\n",
    "            self.add_link(BottleNeck(n_out, n_mid, n_out))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self:\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_class=10, n_blocks=[3, 4, 6, 3]):\n",
    "        super(ResNet, self).__init__()\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 64, 3, 1, 0, True, w)\n",
    "            self.bn2 = L.BatchNormalization(64)\n",
    "            self.res3 = Block(64, 64, 256, n_blocks[0], 1)\n",
    "            self.res4 = Block(256, 128, 512, n_blocks[1], 2)\n",
    "            self.res5 = Block(512, 256, 1024, n_blocks[2], 2)\n",
    "            self.res6 = Block(1024, 512, 2048, n_blocks[3], 2)\n",
    "            self.fc7 = L.Linear(None, n_class)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn2(self.conv1(x)))\n",
    "        h = self.res3(h)\n",
    "        h = self.res4(h)\n",
    "        h = self.res5(h)\n",
    "        h = self.res6(h)\n",
    "        h = F.average_pooling_2d(h, h.shape[2:])\n",
    "        h = self.fc7(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class ResNet50(ResNet):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "        super(ResNet50, self).__init__(n_class, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kYv__Gv8DgC"
   },
   "source": [
    "次に学習データを読み込みます．CIFAR10データセットは中規模な一般物体認識のデータセットであり，chainerでは CIFAR10\n",
    "データセットを取得し，学習するためのフォーマットに変換してくれます．データセットには学習用とテスト用のデータに別れており，それぞれtrain_dataset, test_datasetとします．また，それらには画像データと教師ラベルがあり，それらをtrain_xとtrain_y，test_xとtest_yとします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCv8tqrY76Ug"
   },
   "outputs": [],
   "source": [
    "#train_dataset, test_dataset = chainer.datasets.get_cifar10(scale=255.)\n",
    "train_dataset, test_dataset = chainer.datasets.get_cifar10()\n",
    "mean = np.mean([x for x, _ in train_dataset], axis=(0, 2, 3))\n",
    "std = np.std([x for x, _ in train_dataset], axis=(0, 2, 3))\n",
    "print (len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRrkVN6cdyr_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRHjjCUVdxfd"
   },
   "outputs": [],
   "source": [
    "def transform(  inputs, mean, std, expand_ratio=1.0, crop_size=(32, 32), train=True):\n",
    "    img, label = inputs\n",
    "    img = img.copy()\n",
    "\n",
    "    if train:\n",
    "        # Random flip\n",
    "        img = transforms.random_flip(img, x_random=True)\n",
    "        # Random expand\n",
    "        if expand_ratio > 1:\n",
    "            img = transforms.random_expand(img, max_ratio=expand_ratio)\n",
    "        # Random crop\n",
    "        if tuple(crop_size) != (32, 32):\n",
    "            img = transforms.random_crop(img, tuple(crop_size))\n",
    "\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1pzZryyenRk"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qQoFXAvenrH"
   },
   "outputs": [],
   "source": [
    "train_transform = partial( transform, mean=mean, std=std, expand_ratio=1.2, crop_size=[28, 28], train=True)\n",
    "valid_transform = partial(transform, mean=mean, std=std, train=False)\n",
    "\n",
    "train_dataset = TransformDataset(train_dataset, train_transform)\n",
    "test_dataset = TransformDataset(test_dataset, valid_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hYTV75a79OS"
   },
   "source": [
    "畳み込みネットワークモデルを定義します．ここでは，GPUで学習を行うために，modelをGPUに送るto_gpu関数を利用しています．学習を行う際の最適化方法としてモーメンタムをもつ確率的勾配降下法（SGD）を利用します．SGDには学習率を指定します．その最適化方法に定義したネットワークモデルを与えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rdmJMVY75vq"
   },
   "outputs": [],
   "source": [
    "gpu_id = 0 \n",
    "xp = cuda.cupy\n",
    "model = ResNet50()\n",
    "model.to_gpu(gpu_id)\n",
    "\n",
    "learnrate = 0.01\n",
    "optimizer = chainer.optimizers.MomentumSGD(learnrate)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUNa9Xe79vAG"
   },
   "source": [
    "１回の誤差を算出するデータ数（ミニバッチサイズ）を128，学習エポック数を100とします．MNISTの学習データサイズを取得し，１エポック内における更新回数を求めます．学習データは毎エポックでランダムに利用するため，numpyのpermutationという関数を利用します．各更新において，学習用データと教師データをそれぞれxとtとし，to_gpu関数でGPUに転送します．学習モデルにxを与えて各クラスの確率yを取得します．各クラスの確率yと教師ラベルtとの誤差をsoftmax coross entropy誤差関数で算出します．また，認識精度も算出します．そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68RE3RTa76-W"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epoch_num = 100\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "        dataset_x = []\n",
    "        dataset_y =[]\n",
    "        for train in train_dataset:\n",
    "            dataset_x.append(train[0])\n",
    "            dataset_y.append(train[1])\n",
    "  \n",
    "        train_x = xp.asarray(dataset_x, xp.float32)\n",
    "        train_y = xp.asarray(dataset_y, xp.int32)\n",
    "        print (train_x.shape, train_y.shape)\n",
    "  \n",
    "        train_data_num = train_x.shape[0]\n",
    "        iter_one_epoch = int(train_x.shape[0]/batch_size)\n",
    "        sum_loss = 0\n",
    "        sum_accuracy = 0\n",
    "        if (epoch+1) % 25 == 0 :\n",
    "            optimizer.lr *= 0.5\n",
    "        perm = xp.random.permutation(train_data_num)\n",
    "        for i in range(0, train_data_num, batch_size):\n",
    "                x = Variable(cuda.to_gpu(train_x[perm[i:i+batch_size]]))\n",
    "                t = Variable(cuda.to_gpu(train_y[perm[i:i+batch_size]]))\n",
    "                y = model(x)        \n",
    "                model.zerograds()\n",
    "                loss = F.softmax_cross_entropy(y, t)\n",
    "                acc = F.accuracy(y, t)\n",
    "                loss.backward()\n",
    "                optimizer.update()\n",
    "                sum_loss += loss.data\n",
    "                sum_accuracy += acc.data\n",
    "        print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch+1, sum_loss*batch_size/train_data_num, sum_accuracy*batch_size/train_data_num))\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            chainer.serializers.save_hdf5('caifr10_aug.npz', model, compression=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6OZY7GgDBPl"
   },
   "source": [
    "学習できたネットワークモデルを利用して評価を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03_8Wh5V9F6V"
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "dataset_x = []\n",
    "dataset_y =[]\n",
    "for test in test_dataset:\n",
    "    dataset_x.append(test[0])\n",
    "    dataset_y.append(test[1])\n",
    "test_x = xp.asarray(dataset_x, xp.float32)\n",
    "test_y = xp.asarray(dataset_y, xp.int32)\n",
    "print (test_x.shape, test_y.shape)\n",
    "  \n",
    "test_data_num = test_x.shape[0]\n",
    "for i in range(0, test_data_num):\n",
    "       x = Variable(cuda.to_gpu(test_x[i].reshape(1,3,32,32)))\n",
    "       t = test_y[i]\n",
    "       y = model(x)        \n",
    "       y = np.argmax(y.data[0])\n",
    "       if t == y:\n",
    "           cnt += 1\n",
    "print(\"test accuracy: {}\".format(cnt/test_data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38K6sUHw8X8f"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "for data in test_dataset[:100]:\n",
    "    bimg = data[0].copy()\n",
    "    label = data[1]\n",
    "    bimg *=255.\n",
    "    img = bimg.transpose(1,2,0)\n",
    "    #display_cv_image(img, '.png')\n",
    "    decoded_bytes = cv2.imencode('.png', img)[1].tobytes()\n",
    "    display(Image(data=decoded_bytes)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxO7tnfWDHMJ"
   },
   "source": [
    "## 課題　\n",
    "###以下の課題に取り組みましょう\n",
    "1  ネットワーク構造を変えて実験しましょう\n",
    "\n",
    "2  最適化の方法をAdam以外に変えて実験しましょう\n",
    "\n",
    "3  エポック数やミニバッチサイズを変えて実験しましょう\n",
    "\n",
    "4  GPUの有無による速度の差を比較しましょう\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "05_ResNet_chainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
