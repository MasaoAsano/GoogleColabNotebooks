{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ed68qTOyYTNI"
   },
   "source": [
    "pytorchをインストールします．\n",
    "CUDAのバージョンは8.0，pytorchのバージョンは0.4.0です．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5787,
     "status": "ok",
     "timestamp": 1526805148607,
     "user": {
      "displayName": "Takayoshi Yamashita",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109048018733476271678"
     },
     "user_tz": -540
    },
    "id": "Dnt0rTj0KGk7",
    "outputId": "a000d831-35c7-487a-c8e5-a5ab225451dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==0.4.0 from http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl \n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ER0ztBYkY0zN"
   },
   "source": [
    "pytorchに必要なパッケージ類をインポートします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EbQw7lBBKRfA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1526817834511,
     "user": {
      "displayName": "Takayoshi Yamashita",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109048018733476271678"
     },
     "user_tz": -540
    },
    "id": "Oogn_oxFbIbj",
    "outputId": "04ddf4d7-4e19-4730-f81c-a895aa761fc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0ePqcE9Y--a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1c0iUg1qKhjF"
   },
   "outputs": [],
   "source": [
    "num_epochs    = 300\n",
    "batch_size    = 128\n",
    "learning_rate = 1e-3\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LXZ8MDCdKk5L"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(),  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2453,
     "status": "ok",
     "timestamp": 1526817842281,
     "user": {
      "displayName": "Takayoshi Yamashita",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109048018733476271678"
     },
     "user_tz": -540
    },
    "id": "jNb7J9CXkxxr",
    "outputId": "89135101-1650-4ad7-d193-05d7178c3a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = dsets.CIFAR10(root='./data', train=True,  download=True, transform=transform_train)\n",
    "testset =  dsets.CIFAR10(root='./data', train=False,download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3NyfW6C8dJYJ"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "b6nGBeKDRyhB"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-pgSR5jCSsQU"
   },
   "outputs": [],
   "source": [
    "#ResNet18\n",
    "net = ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "#ResNet34\n",
    "#net = ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "#ResNet50\n",
    "#net = ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "#ResNet101\n",
    "#net = ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "#ResNet152\n",
    "#net = ResNet(Bottleneck, [3,8,36,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n_PVdRrKLAcu"
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y7XwNYNPLGA2"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 225], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1955
    },
    "colab_type": "code",
    "id": "xttrh4TaLQY7",
    "outputId": "d513c017-3dde-4437-948f-64d2725f96d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.00343 | Acc: 84.638% (42319/50000)\n",
      "1 Loss: 0.00322 | Acc: 85.578% (42789/50000)\n",
      "2 Loss: 0.00308 | Acc: 86.192% (43096/50000)\n",
      "3 Loss: 0.00289 | Acc: 87.186% (43593/50000)\n",
      "4 Loss: 0.00274 | Acc: 87.746% (43873/50000)\n",
      "5 Loss: 0.00262 | Acc: 88.354% (44177/50000)\n",
      "6 Loss: 0.00251 | Acc: 88.798% (44399/50000)\n",
      "7 Loss: 0.00237 | Acc: 89.478% (44739/50000)\n",
      "8 Loss: 0.00228 | Acc: 89.702% (44851/50000)\n",
      "9 Loss: 0.00216 | Acc: 90.356% (45178/50000)\n",
      "10 Loss: 0.00209 | Acc: 90.784% (45392/50000)\n",
      "11 Loss: 0.00199 | Acc: 91.186% (45593/50000)\n",
      "12 Loss: 0.00194 | Acc: 91.250% (45625/50000)\n",
      "13 Loss: 0.00184 | Acc: 91.604% (45802/50000)\n",
      "14 Loss: 0.00175 | Acc: 92.148% (46074/50000)\n",
      "15 Loss: 0.00167 | Acc: 92.466% (46233/50000)\n",
      "16 Loss: 0.00166 | Acc: 92.612% (46306/50000)\n",
      "17 Loss: 0.00158 | Acc: 92.836% (46418/50000)\n",
      "18 Loss: 0.00147 | Acc: 93.390% (46695/50000)\n",
      "19 Loss: 0.00146 | Acc: 93.400% (46700/50000)\n",
      "20 Loss: 0.00135 | Acc: 93.936% (46968/50000)\n",
      "21 Loss: 0.00136 | Acc: 93.742% (46871/50000)\n",
      "22 Loss: 0.00126 | Acc: 94.430% (47215/50000)\n",
      "23 Loss: 0.00122 | Acc: 94.408% (47204/50000)\n",
      "24 Loss: 0.00119 | Acc: 94.682% (47341/50000)\n",
      "25 Loss: 0.00115 | Acc: 94.786% (47393/50000)\n",
      "26 Loss: 0.00109 | Acc: 95.146% (47573/50000)\n",
      "27 Loss: 0.00102 | Acc: 95.426% (47713/50000)\n",
      "28 Loss: 0.00104 | Acc: 95.254% (47627/50000)\n",
      "29 Loss: 0.00096 | Acc: 95.582% (47791/50000)\n",
      "30 Loss: 0.00094 | Acc: 95.762% (47881/50000)\n",
      "31 Loss: 0.00091 | Acc: 95.898% (47949/50000)\n",
      "32 Loss: 0.00090 | Acc: 96.008% (48004/50000)\n",
      "33 Loss: 0.00086 | Acc: 96.152% (48076/50000)\n",
      "34 Loss: 0.00084 | Acc: 96.186% (48093/50000)\n",
      "35 Loss: 0.00077 | Acc: 96.570% (48285/50000)\n",
      "36 Loss: 0.00077 | Acc: 96.538% (48269/50000)\n",
      "37 Loss: 0.00074 | Acc: 96.636% (48318/50000)\n",
      "38 Loss: 0.00073 | Acc: 96.660% (48330/50000)\n",
      "39 Loss: 0.00067 | Acc: 97.006% (48503/50000)\n",
      "40 Loss: 0.00066 | Acc: 97.072% (48536/50000)\n",
      "41 Loss: 0.00067 | Acc: 96.932% (48466/50000)\n",
      "42 Loss: 0.00064 | Acc: 97.052% (48526/50000)\n",
      "43 Loss: 0.00062 | Acc: 97.236% (48618/50000)\n",
      "44 Loss: 0.00057 | Acc: 97.472% (48736/50000)\n",
      "45 Loss: 0.00056 | Acc: 97.400% (48700/50000)\n",
      "46 Loss: 0.00057 | Acc: 97.452% (48726/50000)\n",
      "47 Loss: 0.00054 | Acc: 97.584% (48792/50000)\n",
      "48 Loss: 0.00054 | Acc: 97.556% (48778/50000)\n",
      "49 Loss: 0.00050 | Acc: 97.702% (48851/50000)\n",
      "50 Loss: 0.00049 | Acc: 97.740% (48870/50000)\n",
      "51 Loss: 0.00047 | Acc: 97.864% (48932/50000)\n",
      "52 Loss: 0.00047 | Acc: 97.878% (48939/50000)\n",
      "53 Loss: 0.00045 | Acc: 97.990% (48995/50000)\n",
      "54 Loss: 0.00044 | Acc: 98.008% (49004/50000)\n",
      "55 Loss: 0.00046 | Acc: 97.926% (48963/50000)\n",
      "56 Loss: 0.00041 | Acc: 98.142% (49071/50000)\n",
      "57 Loss: 0.00039 | Acc: 98.244% (49122/50000)\n",
      "58 Loss: 0.00039 | Acc: 98.226% (49113/50000)\n",
      "59 Loss: 0.00038 | Acc: 98.360% (49180/50000)\n",
      "60 Loss: 0.00039 | Acc: 98.258% (49129/50000)\n",
      "61 Loss: 0.00036 | Acc: 98.332% (49166/50000)\n",
      "62 Loss: 0.00035 | Acc: 98.424% (49212/50000)\n",
      "63 Loss: 0.00038 | Acc: 98.320% (49160/50000)\n",
      "64 Loss: 0.00035 | Acc: 98.518% (49259/50000)\n",
      "65 Loss: 0.00034 | Acc: 98.424% (49212/50000)\n",
      "66 Loss: 0.00032 | Acc: 98.634% (49317/50000)\n",
      "67 Loss: 0.00033 | Acc: 98.562% (49281/50000)\n",
      "68 Loss: 0.00031 | Acc: 98.636% (49318/50000)\n",
      "69 Loss: 0.00028 | Acc: 98.756% (49378/50000)\n",
      "70 Loss: 0.00029 | Acc: 98.744% (49372/50000)\n",
      "71 Loss: 0.00028 | Acc: 98.686% (49343/50000)\n",
      "72 Loss: 0.00028 | Acc: 98.714% (49357/50000)\n",
      "73 Loss: 0.00029 | Acc: 98.674% (49337/50000)\n",
      "74 Loss: 0.00030 | Acc: 98.638% (49319/50000)\n",
      "75 Loss: 0.00027 | Acc: 98.800% (49400/50000)\n",
      "76 Loss: 0.00025 | Acc: 98.940% (49470/50000)\n",
      "77 Loss: 0.00024 | Acc: 98.926% (49463/50000)\n",
      "78 Loss: 0.00026 | Acc: 98.782% (49391/50000)\n",
      "79 Loss: 0.00025 | Acc: 98.882% (49441/50000)\n",
      "80 Loss: 0.00025 | Acc: 98.982% (49491/50000)\n",
      "81 Loss: 0.00025 | Acc: 98.922% (49461/50000)\n",
      "82 Loss: 0.00024 | Acc: 98.948% (49474/50000)\n",
      "83 Loss: 0.00025 | Acc: 98.922% (49461/50000)\n",
      "84 Loss: 0.00025 | Acc: 98.882% (49441/50000)\n",
      "85 Loss: 0.00025 | Acc: 98.938% (49469/50000)\n",
      "86 Loss: 0.00021 | Acc: 99.072% (49536/50000)\n",
      "87 Loss: 0.00022 | Acc: 98.994% (49497/50000)\n",
      "88 Loss: 0.00024 | Acc: 98.912% (49456/50000)\n",
      "89 Loss: 0.00023 | Acc: 98.994% (49497/50000)\n",
      "90 Loss: 0.00018 | Acc: 99.198% (49599/50000)\n",
      "91 Loss: 0.00019 | Acc: 99.150% (49575/50000)\n",
      "92 Loss: 0.00020 | Acc: 99.090% (49545/50000)\n",
      "93 Loss: 0.00019 | Acc: 99.196% (49598/50000)\n",
      "94 Loss: 0.00018 | Acc: 99.222% (49611/50000)\n",
      "95 Loss: 0.00019 | Acc: 99.148% (49574/50000)\n",
      "96 Loss: 0.00019 | Acc: 99.120% (49560/50000)\n",
      "97 Loss: 0.00019 | Acc: 99.186% (49593/50000)\n",
      "98 Loss: 0.00020 | Acc: 99.164% (49582/50000)\n",
      "99 Loss: 0.00017 | Acc: 99.282% (49641/50000)\n",
      "100 Loss: 0.00016 | Acc: 99.286% (49643/50000)\n",
      "101 Loss: 0.00015 | Acc: 99.368% (49684/50000)\n",
      "102 Loss: 0.00017 | Acc: 99.180% (49590/50000)\n",
      "103 Loss: 0.00017 | Acc: 99.250% (49625/50000)\n",
      "104 Loss: 0.00016 | Acc: 99.284% (49642/50000)\n",
      "105 Loss: 0.00016 | Acc: 99.314% (49657/50000)\n",
      "106 Loss: 0.00015 | Acc: 99.338% (49669/50000)\n",
      "107 Loss: 0.00013 | Acc: 99.454% (49727/50000)\n",
      "108 Loss: 0.00015 | Acc: 99.388% (49694/50000)\n",
      "109 Loss: 0.00015 | Acc: 99.370% (49685/50000)\n",
      "110 Loss: 0.00015 | Acc: 99.350% (49675/50000)\n",
      "111 Loss: 0.00016 | Acc: 99.358% (49679/50000)\n",
      "112 Loss: 0.00014 | Acc: 99.402% (49701/50000)\n",
      "113 Loss: 0.00015 | Acc: 99.330% (49665/50000)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    scheduler.step()\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.cuda())\n",
    "        labels =  Variable(labels.cuda())\n",
    "\n",
    "        optimizer.zero_grad()               \n",
    "        outputs = net(images)              \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()                           \n",
    "        optimizer.step()                          \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()     \n",
    "    print( epoch, 'Loss: %.5f | Acc: %.3f%% (%d/%d)' % (train_loss/((len(train_loader) * batch_size)), 100.*correct/total, correct, total))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2196,
     "status": "ok",
     "timestamp": 1526804829673,
     "user": {
      "displayName": "Takayoshi Yamashita",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109048018733476271678"
     },
     "user_tz": -540
    },
    "id": "O7BAsEP-anwq",
    "outputId": "dea396d6-20bd-40d8-bd8f-5349afa2f45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.01133 | Acc: 59.770% (5977/10000)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    inputs, targets = inputs.to(\"cuda\"), targets.to(\"cuda\")\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = outputs.max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "print( 'Loss: %.5f | Acc: %.3f%% (%d/%d)' % (test_loss/((len(test_loader)*100)), 100.*correct/total, correct, total))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v6B-IUe6at-t"
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'fnn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bWgKcqoJcL9A"
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "      \n",
    "      \n",
    "#net = VGG('VGG11')\n",
    "#net = VGG('VGG13')\n",
    "net = VGG('VGG16')\n",
    "#net = VGG('VGG19')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k2NO1g0-c1rU"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "      \n",
    "net = LeNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yEmZMSZfdE3c"
   },
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1,y2,y3,y4], 1)\n",
    "\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_layers(x)\n",
    "        out = self.a3(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a4(out)\n",
    "        out = self.b4(out)\n",
    "        out = self.c4(out)\n",
    "        out = self.d4(out)\n",
    "        out = self.e4(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "net = GoogLeNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RSHed-RwdUoQ"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "        # SE layers\n",
    "        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n",
    "        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Squeeze\n",
    "        w = F.avg_pool2d(out, out.size(2))\n",
    "        w = F.relu(self.fc1(w))\n",
    "        w = F.sigmoid(self.fc2(w))\n",
    "        # Excitation\n",
    "        out = out * w  # New broadcasting feature from v0.2!\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "        # SE layers\n",
    "        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "\n",
    "        # Squeeze\n",
    "        w = F.avg_pool2d(out, out.size(2))\n",
    "        w = F.relu(self.fc1(w))\n",
    "        w = F.sigmoid(self.fc2(w))\n",
    "        # Excitation\n",
    "        out = out * w\n",
    "\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class SENet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(SENet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "      \n",
    "\n",
    "      \n",
    "net =SENet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_5eOG-SNdj-H"
   },
   "outputs": [],
   "source": [
    "class ShuffleBlock(nn.Module):\n",
    "    def __init__(self, groups):\n",
    "        super(ShuffleBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n",
    "        N,C,H,W = x.size()\n",
    "        g = self.groups\n",
    "        return x.view(N,g,C/g,H,W).permute(0,2,1,3,4).contiguous().view(N,C,H,W)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, groups):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        mid_planes = out_planes/4\n",
    "        g = 1 if in_planes==24 else groups\n",
    "        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
    "        self.shuffle1 = ShuffleBlock(groups=g)\n",
    "        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_planes)\n",
    "        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 2:\n",
    "            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.shuffle1(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        res = self.shortcut(x)\n",
    "        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ShuffleNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ShuffleNet, self).__init__()\n",
    "        out_planes = cfg['out_planes']\n",
    "        num_blocks = cfg['num_blocks']\n",
    "        groups = cfg['groups']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.in_planes = 24\n",
    "        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n",
    "        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n",
    "        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n",
    "        self.linear = nn.Linear(out_planes[2], 10)\n",
    "\n",
    "    def _make_layer(self, out_planes, num_blocks, groups):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = 2 if i == 0 else 1\n",
    "            cat_planes = self.in_planes if i == 0 else 0\n",
    "            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n",
    "            self.in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "      \n",
    "#ShuffleNetG2      \n",
    "cfg = {\n",
    "    'out_planes': [200,400,800],\n",
    "    'num_blocks': [4,8,4],\n",
    "    'groups': 2\n",
    "}\n",
    "\n",
    "#ShuffleNetG3      \n",
    "#cfg = {\n",
    "#    'out_planes': [240,480,960],\n",
    "#    'num_blocks': [4,8,4],\n",
    "#    'groups': 3\n",
    "#}\n",
    "\n",
    "\n",
    "net = ShuffleNet(cfg)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XW3yZZVoeLEj"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Grouped convolution block.'''\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        group_width = cardinality * bottleneck_width\n",
    "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*group_width:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*group_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
    "        # self.layer4 = self._make_layer(num_blocks[3], 2)\n",
    "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
    "\n",
    "    def _make_layer(self, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
    "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
    "        # Increase bottleneck_width by 2 after each stage.\n",
    "        self.bottleneck_width *= 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "      \n",
    "      \n",
    "#ResNeXt29_2x64d():\n",
    "net = ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n",
    "\n",
    "#ResNeXt29_4x64d():\n",
    "#net = ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n",
    "\n",
    "#ResNeXt29_8x64d():\n",
    "#net = ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n",
    "\n",
    "#ResNeXt29_32x4d():\n",
    "#net = ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aVJ8lpJiefeH"
   },
   "outputs": [],
   "source": [
    "class SepConv(nn.Module):\n",
    "    '''Separable Convolution.'''\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride):\n",
    "        super(SepConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes,\n",
    "                               kernel_size, stride,\n",
    "                               padding=(kernel_size-1)//2,\n",
    "                               bias=False, groups=in_planes)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn1(self.conv1(x))\n",
    "\n",
    "\n",
    "class CellA(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(CellA, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n",
    "        if stride==2:\n",
    "            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.sep_conv1(x)\n",
    "        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n",
    "        if self.stride==2:\n",
    "            y2 = self.bn1(self.conv1(y2))\n",
    "        return F.relu(y1+y2)\n",
    "\n",
    "class CellB(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(CellB, self).__init__()\n",
    "        self.stride = stride\n",
    "        # Left branch\n",
    "        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n",
    "        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)\n",
    "        # Right branch\n",
    "        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)\n",
    "        if stride==2:\n",
    "            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        # Reduce channels\n",
    "        self.conv2 = nn.Conv2d(2*out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Left branch\n",
    "        y1 = self.sep_conv1(x)\n",
    "        y2 = self.sep_conv2(x)\n",
    "        # Right branch\n",
    "        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n",
    "        if self.stride==2:\n",
    "            y3 = self.bn1(self.conv1(y3))\n",
    "        y4 = self.sep_conv3(x)\n",
    "        # Concat & reduce channels\n",
    "        b1 = F.relu(y1+y2)\n",
    "        b2 = F.relu(y3+y4)\n",
    "        y = torch.cat([b1,b2], 1)\n",
    "        return F.relu(self.bn2(self.conv2(y)))\n",
    "\n",
    "class PNASNet(nn.Module):\n",
    "    def __init__(self, cell_type, num_cells, num_planes):\n",
    "        super(PNASNet, self).__init__()\n",
    "        self.in_planes = num_planes\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_planes)\n",
    "\n",
    "        self.layer1 = self._make_layer(num_planes, num_cells=6)\n",
    "        self.layer2 = self._downsample(num_planes*2)\n",
    "        self.layer3 = self._make_layer(num_planes*2, num_cells=6)\n",
    "        self.layer4 = self._downsample(num_planes*4)\n",
    "        self.layer5 = self._make_layer(num_planes*4, num_cells=6)\n",
    "\n",
    "        self.linear = nn.Linear(num_planes*4, 10)\n",
    "\n",
    "    def _make_layer(self, planes, num_cells):\n",
    "        layers = []\n",
    "        for _ in range(num_cells):\n",
    "            layers.append(self.cell_type(self.in_planes, planes, stride=1))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _downsample(self, planes):\n",
    "        layer = self.cell_type(self.in_planes, planes, stride=2)\n",
    "        self.in_planes = planes\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = self.linear(out.view(out.size(0), -1))\n",
    "        return out\n",
    "\n",
    "\n",
    "#PNASNetA():\n",
    "net = PNASNet(CellA, num_cells=6, num_planes=44)\n",
    "\n",
    "#PNASNetB():\n",
    "#net = PNASNet(CellB, num_cells=6, num_planes=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hv04KatYfAd6"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n",
    "    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "      \n",
    "net = MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bRYkWJnEfJip"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''expand + depthwise + pointwise'''\n",
    "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        planes = expansion * in_planes\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    # (expansion, out_planes, num_blocks, stride)\n",
    "    cfg = [(1,  16, 1, 1),\n",
    "           (6,  24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n",
    "           (6,  32, 3, 2),\n",
    "           (6,  64, 4, 2),\n",
    "           (6,  96, 3, 1),\n",
    "           (6, 160, 3, 2),\n",
    "           (6, 320, 1, 1)]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(1280)\n",
    "        self.linear = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1]*(num_blocks-1)\n",
    "            for stride in strides:\n",
    "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
    "                in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "      \n",
    "      \n",
    "net = MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vsnN4gbPfblC"
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.out_planes = out_planes\n",
    "        self.dense_depth = dense_depth\n",
    "\n",
    "        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if first_layer:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes+dense_depth)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        x = self.shortcut(x)\n",
    "        d = self.out_planes\n",
    "        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DPN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DPN, self).__init__()\n",
    "        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n",
    "        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.last_planes = 64\n",
    "        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n",
    "        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n",
    "        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n",
    "        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n",
    "        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 10)\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for i,stride in enumerate(strides):\n",
    "            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n",
    "            self.last_planes = out_planes + (i+2) * dense_depth\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#DPN26():\n",
    "cfg = {\n",
    "    'in_planes': (96,192,384,768),\n",
    "    'out_planes': (256,512,1024,2048),\n",
    "    'num_blocks': (2,2,2,2),\n",
    "    'dense_depth': (16,32,24,128)\n",
    "}\n",
    "net = DPN(cfg)\n",
    "\n",
    "#DPN92():\n",
    "#cfg = {\n",
    "#    'in_planes': (96,192,384,768),\n",
    "#    'out_planes': (256,512,1024,2048),\n",
    "#    'num_blocks': (3,4,20,3),\n",
    "#    'dense_depth': (16,32,24,128)\n",
    "#}\n",
    "#net = DPN(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CHTypfmNfzod"
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "#DenseNet121():\n",
    "#net = DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
    "\n",
    "# DenseNet169():\n",
    "#net = DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
    "\n",
    "# DenseNet201():\n",
    "#net = DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
    "\n",
    "#DenseNet161():\n",
    "#net =  DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n",
    "\n",
    "# densenet_cifar():\n",
    "net DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "03_pytorch_ResNet_CIFAR10.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
