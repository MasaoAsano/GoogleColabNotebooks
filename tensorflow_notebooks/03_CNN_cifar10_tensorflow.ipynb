{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_CNN_cifar10_tensorflow.ipynb","version":"0.3.2","provenance":[{"file_id":"1d0ZZZxxFS1QUfMtCZC_wTMOKkmvwRtSM","timestamp":1534333622972},{"file_id":"1crLbZF0eepPiNP9yhc4HcO_LBPQ1_jy4","timestamp":1534325989751}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"6KXGk-IwxKyr","colab_type":"text"},"cell_type":"markdown","source":["#TensorFlowによる畳み込みニューラルネットワーク(CIFAR10)"]},{"metadata":{"id":"wiXv2wIayBIp","colab_type":"text"},"cell_type":"markdown","source":["TensorFlowでニューラルネットワークのプログラムを実装する方法として，セッションを準備して実行する方法，Estimatorを利用する方法，Eagerを利用する方法があります．ここでは，まずセッションを準備する方法について説明します．また，学習および評価データセットとしてCIFAR10を用います．CIFAR10は，１０クラスの一般物体認識のデータセットです．データセットの準備の方法も含めて説明します．"]},{"metadata":{"id":"rJF5Dboz0vdb","colab_type":"text"},"cell_type":"markdown","source":["必要なパッケージをインポートします．\n","また，GPUが利用可能かを確認します．"]},{"metadata":{"id":"p3TDtyQgxApb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0b69c46c-1b78-457a-8486-ab11e322c1c6","executionInfo":{"status":"ok","timestamp":1534470109227,"user_tz":-540,"elapsed":514,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["import os\n","import sys\n","import tarfile\n","from six.moves import urllib\n","import numpy as np\n","import pickle\n","import glob\n","import tensorflow as tf\n","\n","tf.test.gpu_device_name()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"YzZcm4xOyCJ9","colab_type":"text"},"cell_type":"markdown","source":["CIFARデータセットをダウンロードします．ここでは，pythonでデータを扱うためにpython用に用意されたデータセットを利用します．"]},{"metadata":{"id":"W9IP2sbRx-K9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"01d2ac58-cabd-40a7-fb65-6f83febfedac","executionInfo":{"status":"ok","timestamp":1534470173146,"user_tz":-540,"elapsed":62094,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["dest_directory = \"./\"\n","DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n","\n","if not os.path.exists(dest_directory):\n","    os.makedirs(dest_directory)\n","\n","filename = DATA_URL.split('/')[-1]\n","filepath = os.path.join(dest_directory, filename)\n","\n","if not os.path.exists(filepath):\n","    def _progress(count, block_size, total_size):\n","      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,  float(count * block_size) / float(total_size) * 100.0))\n","      sys.stdout.flush()\n","    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n","    print()\n","    statinfo = os.stat(filepath)\n","    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n","\n","extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-py')\n","if not os.path.exists(extracted_dir_path):\n","  tarfile.open(filepath, 'r:gz').extractall(dest_directory)"],"execution_count":4,"outputs":[{"output_type":"stream","text":[">> Downloading cifar-10-python.tar.gz 100.0%\n","Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n"],"name":"stdout"}]},{"metadata":{"id":"eN6jzxk144ZW","colab_type":"text"},"cell_type":"markdown","source":["ダウンロードしたCIFAR10データセットを読み込みます．ダウンロードしたデータは学習用に5つ，評価用に１つのファイルがあり，バイト形式になっています．これをnumpy形式のデータに変換します．各画像は32x32ピクセルのカラー画像です．よって32x32=1024バイトごとデータをチャンネルとして重ね合わせることでカラー画像にすることができます．教師データは0から9のクラスラベルです．学習データとその教師ラベルをそれぞれX_train，y_trainとします．また，評価データとその教師ラベルをX_test，y_testとします．\n","\n","そして，各データを平均０，分散１になるように正規化を行います．\n","これは，画像のばらつきを抑える前処理として利用されている方法です．"]},{"metadata":{"id":"q0afEE2CjItx","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_dataset(dirpath='cifar-10-batches-py'):\n","    X, y = [], []\n","    # take data from the data batch\n","    for path in glob.glob('%s/data_batch_*' % dirpath):\n","        with open(path, 'rb') as f:\n","            batch = pickle.load(f, encoding='bytes')\n","        # append all data and labels from the 5 data betch\n","        X.append(batch[b'data'])\n","        y.append(batch[b'labels'])\n","    # devide by 255 for making value 0 to 1\n","    X = np.concatenate(X) /np.float32(255)\n","    # making labels as int\n","    y = np.concatenate(y).astype(np.int64)\n","    #seperate in to RGB colors\n","    X = np.dstack((X[:, :1024], X[:, 1024:2048], X[:, 2048:]))\n","    # reshape data into 4D tensor with compatible to CNN model\n","    X_train = X.reshape((X.shape[0], 32, 32, 3))\n","    y_train = y.reshape((y.shape[0]))\n","        \n","    # load test set\n","    path = '%s/test_batch' % dirpath\n","    with open(path, 'rb') as f:\n","        batch = pickle.load(f, encoding='bytes')\n","    X_test = batch[b'data'] /np.float32(255)\n","    X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:]))\n","    X_test = X_test.reshape((X_test.shape[0], 32, 32, 3))\n","    y_t = np.array(batch[b'labels'], dtype=np.int64)\n","    y_test = y_t.reshape((y_t.shape[0]))\n","\n","    # normalize to zero mean and unity variance\n","    offset = np.mean(X_train, 0)\n","    scale = np.std(X_train, 0).clip(min=1)\n","    X_train = (X_train - offset) / scale\n","    X_test = (X_test - offset) / scale\n","    return X_train, y_train, X_test, y_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vnk-JucmyHCL","colab_type":"text"},"cell_type":"markdown","source":["ネットワークのパラメータ(重み，バイアス)を初期化します．"]},{"metadata":{"id":"0ZK5r2DNyGLq","colab_type":"code","colab":{}},"cell_type":"code","source":["weights = {\n","    'w1': tf.Variable(tf.random_normal([5,5,3,120],stddev = 0.1)),\n","    'w2': tf.Variable(tf.random_normal([5,5,120,60],stddev = 0.1)),\n","    'w3': tf.Variable(tf.random_normal([4,4,60,30],stddev = 0.1)),\n","    'w4': tf.Variable(tf.random_normal([4*4*30,30],stddev = 0.1)),\n","    'w5': tf.Variable(tf.random_normal([30,10],stddev = 0.1))\n","}\n","\n","biases = {\n","    'b1': tf.Variable(tf.random_normal([120],stddev = 0.1)),\n","    'b2': tf.Variable(tf.random_normal([60],stddev = 0.1)),\n","    'b3': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n","    'b4': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n","    'b5': tf.Variable(tf.random_normal([10],stddev = 0.1))\n","}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2MbNYbWwbjWy","colab_type":"code","colab":{}},"cell_type":"code","source":["weights = {\n","    'w1': tf.Variable(tf.random_normal([5,5,3,32],stddev = 5e-2)),\n","    'w2': tf.Variable(tf.random_normal([5,5,32,64],stddev =5e-2)),\n","    'w3': tf.Variable(tf.random_normal([5,5,64,64],stddev = 5e-2)),\n","    'w4': tf.Variable(tf.random_normal([4*4*64,384],stddev = 0.04)),\n","    'w5': tf.Variable(tf.random_normal([384,10],stddev = 0.04))\n","}\n","\n","biases = {\n","    'b1': tf.Variable(tf.random_normal([32],stddev = 0.1)),\n","    'b2': tf.Variable(tf.random_normal([64],stddev = 0.1)),\n","    'b3': tf.Variable(tf.random_normal([64],stddev = 0.1)),\n","    'b4': tf.Variable(tf.random_normal([384],stddev = 0.1)),\n","    'b5': tf.Variable(tf.random_normal([10],stddev = 0.1))\n","}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e698krXc3fo6","colab_type":"text"},"cell_type":"markdown","source":["ネットワークモデルをcnnとして定義します．入力(input_val)は，32x32x3というカラー画像とします．１層目の畳み込み層のフィルタは，x['w1']とし，具体的なフィルタサイズとフィルタ数はパラメータの初期化時に決めた値とします．畳み込み演算を行うストライドは[1,1,1,1]とし，ゼロパディングを行う設定(padding='SAME')としています．そして，バイアス(b['b'])の値を加算し，活性化関数reluを適用した値をconv1とします．\n","次に，最大値プーリングを行います．\n","２層目以降の畳み込みも同様に設定を行います．\n","全結層のユニット数もパラメータの初期化時に決めた値とします．\n","最後にソフトマックス関数を適用して各クラスの確率を求めます．\n","戻り値は，全結合層の値とクラス確率とします．\n"]},{"metadata":{"id":"DZHwz5ZddCKG","colab_type":"code","colab":{}},"cell_type":"code","source":["def cnn(input_val,w,b):\n","\n","    conv1 = tf.nn.conv2d(input_val,w['w1'],strides = [1,1,1,1], padding = 'SAME')\n","    conv1 = tf.nn.bias_add(conv1,b['b1'])\n","    conv1 = tf.nn.relu(conv1)\n","    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    conv2 = tf.nn.conv2d(pool1,w['w2'],strides = [1,1,1,1], padding = 'SAME')\n","    conv2 = tf.nn.bias_add(conv2,b['b2'])\n","    conv2 = tf.nn.relu(conv2)\n","    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    conv3 = tf.nn.conv2d(pool2,w['w3'],strides = [1,1,1,1], padding = 'SAME')\n","    conv3 = tf.nn.bias_add(conv3,b['b3'])\n","    conv3 = tf.nn.relu(conv3)  \n","    pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    shape = pool3.get_shape().as_list()\n","    dense = tf.reshape(pool3,[-1,shape[1]*shape[2]*shape[3]])\n","    dense1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(dense,w['w4']),b['b4']))\n","    \n","    out = tf.nn.bias_add(tf.matmul(dense1,w['w5']),b['b5'])\n","\n","    softmax = tf.nn.softmax(out)\n","    \n","    return out,softmax\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Q39mHLvyk4Y","colab_type":"text"},"cell_type":"markdown","source":["学習データと教師ラベルを準備します．\n","placeholderは，データを格納する入れ物です．ここでは，入れ物のサイズだけを決めて，具体的な値は実行する時に与えます．"]},{"metadata":{"id":"FI879-osyihG","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder(tf.float32,[None,32,32,3])\n","y = tf.placeholder(tf.int64,[None])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IhMF1nwZ4Tb6","colab_type":"text"},"cell_type":"markdown","source":["ネットワークモデルを定義します．"]},{"metadata":{"id":"5kO6rXPY4Z5U","colab_type":"code","colab":{}},"cell_type":"code","source":["predict,  softmax_out= cnn(x,weights,biases)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xgcL-8mu4dtB","colab_type":"text"},"cell_type":"markdown","source":["誤差関数と最適化の方法を定義します．ここでは，物体認識を行うのでソフトマックスクロスエントロピーを誤差関数とします．最適化の方法には，Adamを利用します．誤差はminimizeで最小化するようにします．"]},{"metadata":{"id":"8atjy0aq5c2T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"19305c30-a846-4880-e2cc-6a3a45597c0e","executionInfo":{"status":"ok","timestamp":1534339271118,"user_tz":-540,"elapsed":1180,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["error = tf.reduce_mean( tf.losses.sparse_softmax_cross_entropy(logits = predict,labels = y))\n","optm = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(error)\n","    \n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-8-4ec0209b29fb>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Zza5fkvU5d8-","colab_type":"text"},"cell_type":"markdown","source":["次に評価を行う準備をします．ネットワークからの出力値が最大となるクラスの番号と教師データが一致するかをequalで判定します．全てのデータに対する結果をまとめてaccuracyを算出します．"]},{"metadata":{"id":"llC2SFfm52E5","colab_type":"code","colab":{}},"cell_type":"code","source":["corr = tf.equal(tf.argmax(predict, 1), y)\n","accuracy = tf.reduce_mean(tf.cast(corr, tf.float32)) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"O9z66BJ37bmj","colab_type":"text"},"cell_type":"markdown","source":["ミニバッチ学習として複数の学習データを渡すための関数iterate_minibatchesを用意します．ここでは，学習データのインデックス番号をシャッフルし，その順番に学習データとその教師データをミニバッチサイズ(batchsize)分戻り値とします．呼び出されるたびに順次異なるデータを渡していきます．"]},{"metadata":{"id":"_8T7o917nsdY","colab_type":"code","colab":{}},"cell_type":"code","source":["def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n","    assert len(inputs) == len(targets)\n","    if shuffle:\n","        indices = np.arange(len(inputs))\n","        np.random.shuffle(indices)\n","    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n","        if shuffle:\n","            excerpt = indices[start_idx:start_idx + batchsize]\n","        else:\n","            excerpt = slice(start_idx, start_idx + batchsize)\n","        yield inputs[excerpt], targets[excerpt]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eZdpwC-861xK","colab_type":"text"},"cell_type":"markdown","source":["学習を実行するために，セッションをtf.Session()で準備します．そして，セッションをsess.runで実行しますが，最初にネットワークのパラメータの初期化を行います．for文内で指定したエポック数分の学習を行います．そして，iterate_minibatchesで取得したミニバッチ分の学習データを利用して学習を行います．最初のsess.run(optm, feed_dict・・・)で，fee_dictとして与えたデータをネットワークに順伝播し，誤差を算出します．そして，sess.run([error, accuracy], feed_dict・・・)で，誤差と精度を算出します．\n","１エポック分の学習が終わると，誤差と精度を画面に表示します．"]},{"metadata":{"id":"XJ0Auo2Gu2TU","colab_type":"code","colab":{}},"cell_type":"code","source":["num_epochs = 300\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    print(\"loading dataset...\")\n","    X_train,y_train, X_test,y_test = load_dataset()\n","    print(\"Starting training...\")\n","    \n","    for epoch in range(num_epochs):\n","        train_err = 0\n","        train_acc = 0\n","        train_batches = 0\n","        for batch in iterate_minibatches(X_train, y_train, 128, shuffle=True):\n","            tt = sess.run(optm,feed_dict = {x: batch[0],y: batch[1]})\n","            err,acc= sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})\n","            \n","            train_err += err\n","            train_acc += acc\n","            train_batches += 1\n","        print(\"Epoch: \",  epoch, \" Error: \", train_err/train_batches, \" Accuracy: \", train_acc/train_batches )    \n","        \n","        if (epoch+1) %10 ==0:\n","            test_err = 0\n","            test_acc = 0\n","            test_batches = 0\n","            for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n","                err, acc = sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})# apply tensor function\n","                test_err += err\n","                test_acc += acc\n","                test_batches += 1\n","            print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n","            print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-tuDB0w8Lga","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"BaRewbjowGkx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1802},"outputId":"b9dc3562-fb8c-497c-e7c4-3c41860ff4df","executionInfo":{"status":"ok","timestamp":1534476483325,"user_tz":-540,"elapsed":1544290,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["import tensorflow as tf\n","import pickle\n","import sys\n","import os\n","import time\n","import numpy as np\n","import glob\n","import cv2\n","\n","def load_dataset(dirpath='cifar-10-batches-py'):\n","    X, y = [], []\n","    # take data from the data batch\n","    for path in glob.glob('%s/data_batch_*' % dirpath):\n","        with open(path, 'rb') as f:\n","            batch = pickle.load(f, encoding='bytes')\n","        # append all data and labels from the 5 data betch\n","        X.append(batch[b'data'])\n","        y.append(batch[b'labels'])\n","    # devide by 255 for making value 0 to 1\n","    X = np.concatenate(X) /np.float32(255)\n","    # making labels as int\n","    y = np.concatenate(y).astype(np.int64)\n","    #seperate in to RGB colors\n","    X = np.dstack((X[:, :1024], X[:, 1024:2048], X[:, 2048:]))\n","    # reshape data into 4D tensor with compatible to CNN model\n","    X_train = X.reshape((X.shape[0], 32, 32, 3))\n","    y_train = y.reshape((y.shape[0]))\n","        \n","    # load test set\n","    path = '%s/test_batch' % dirpath\n","    with open(path, 'rb') as f:\n","        batch = pickle.load(f, encoding='bytes')\n","    X_test = batch[b'data'] /np.float32(255)\n","    X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:]))\n","    X_test = X_test.reshape((X_test.shape[0], 32, 32, 3))\n","    y_t = np.array(batch[b'labels'], dtype=np.int64)\n","    y_test = y_t.reshape((y_t.shape[0]))\n","\n","    # normalize to zero mean and unity variance\n","    offset = np.mean(X_train, 0)\n","    scale = np.std(X_train, 0).clip(min=1)\n","    X_train = (X_train - offset) / scale\n","    X_test = (X_test - offset) / scale\n","    return X_train, y_train, X_test, y_test\n","\n","# this function is used as divide input data and labels in mini batch(batchsize) and also used shuffle to give some randomness to CNN \n","def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n","    assert len(inputs) == len(targets)\n","    # shuffle is used in train the data\n","    if shuffle:\n","        indices = np.arange(len(inputs))\n","        np.random.shuffle(indices)\n","    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n","        if shuffle:\n","            excerpt = indices[start_idx:start_idx + batchsize]\n","        else:\n","            excerpt = slice(start_idx, start_idx + batchsize)\n","        yield inputs[excerpt], targets[excerpt]\n","\n","# Convolution neural network model\n","# {conv(with relu) -> max_pool -> conv(with relu) -> max_pool -> conv(with relu) -> max_pool -> dense layer -> [output(train), softmax(main predictionss)]} \n","def build_model(input_val,w,b):\n","\n","    conv1 = tf.nn.conv2d(input_val,w['w1'],strides = [1,1,1,1], padding = 'SAME')\n","    conv1 = tf.nn.bias_add(conv1,b['b1'])\n","    conv1 = tf.nn.relu(conv1)\n","    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    conv2 = tf.nn.conv2d(pool1,w['w2'],strides = [1,1,1,1], padding = 'SAME')\n","    conv2 = tf.nn.bias_add(conv2,b['b2'])\n","    conv2 = tf.nn.relu(conv2)\n","    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    conv3 = tf.nn.conv2d(pool2,w['w3'],strides = [1,1,1,1], padding = 'SAME')\n","    conv3 = tf.nn.bias_add(conv3,b['b3'])\n","    conv3 = tf.nn.relu(conv3)  \n","    pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    shape = pool3.get_shape().as_list()\n","    dense = tf.reshape(pool3,[-1,shape[1]*shape[2]*shape[3]])\n","    dense1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(dense,w['w4']),b['b4']))\n","    \n","    # used for training the CNN model\n","    out = tf.nn.bias_add(tf.matmul(dense1,w['w5']),b['b5'])\n","\n","    # used after training the CNN\n","    softmax = tf.nn.softmax(out)\n","    \n","#    return out\n","    return out,softmax\n","\n","# main function where network train and predict the output on random image\n","def main_function(num_epochs=100):\n","    \n","    # initialize input data shape and datatype for data and labels\n","    x = tf.placeholder(tf.float32,[None,32,32,3])\n","    y = tf.placeholder(tf.int64,[None])\n","    \n","    # initialize weights for every different layers\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([5,5,3,120],stddev = 0.1)),\n","        'w2': tf.Variable(tf.random_normal([5,5,120,60],stddev = 0.1)),\n","        'w3': tf.Variable(tf.random_normal([4,4,60,30],stddev = 0.1)),\n","        'w4': tf.Variable(tf.random_normal([4*4*30,30],stddev = 0.1)),\n","        'w5': tf.Variable(tf.random_normal([30,10],stddev = 0.1))\n","    }\n","\n","    # initialize biases for every different layers\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([120],stddev = 0.1)),\n","        'b2': tf.Variable(tf.random_normal([60],stddev = 0.1)),\n","        'b3': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n","        'b4': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n","        'b5': tf.Variable(tf.random_normal([10],stddev = 0.1))\n","    }\n","\n","    # call model \n","    predict,  softmax_out= build_model(x,weights,biases)\n","    # whole back propagetion process\n","    error = tf.reduce_mean( tf.losses.sparse_softmax_cross_entropy(logits = predict,labels = y))\n","    optm = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(error)\n","#    corr = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))\n","#    accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n","    \n","    corr = tf.equal(tf.argmax(predict, 1), y)\n","    accuracy = tf.reduce_mean(tf.cast(corr, tf.float32)) \n","    \n","    # initialize tensorflow session\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","    # load dataset \n","    print(\"loading dataset...\")\n","    X_train,y_train, X_test,y_test = load_dataset()\n","    # training will start\n","    print(\"Starting training...\")\n","    \n","    for epoch in range(num_epochs):\n","        train_err = 0\n","        train_acc = 0\n","        train_batches = 0\n","        start_time = time.time()\n","        # devide data into mini batch\n","        for batch in iterate_minibatches(X_train, y_train, 128, shuffle=True):\n","            # this is update weights\n","            tt = sess.run(optm,feed_dict = {x: batch[0],y: batch[1]})\n","            # cost function\n","            err,acc= sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})\n","            \n","            train_err += err\n","            train_acc += acc\n","            train_batches += 1\n","        print(\"Epoch: \",  epoch, \" Error: \", train_err/train_batches, \" Accuracy: \", train_acc/train_batches )    \n","        \n","    # testing using test dataset as per above    \n","    test_err = 0\n","    test_acc = 0\n","    test_batches = 0\n","    for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n","        err, acc = sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})# apply tensor function\n","        test_err += err\n","        test_acc += acc\n","        test_batches += 1\n","    print(\"Final results:\")\n","    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n","    print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n","\n","    sess.close()\n","\n","main_function()"],"execution_count":28,"outputs":[{"output_type":"stream","text":["loading dataset...\n","Starting training...\n","Epoch:  0  Error:  1.769444672877972  Accuracy:  0.32646233974358974\n","Epoch:  1  Error:  1.3689732319269425  Accuracy:  0.4936298076923077\n","Epoch:  2  Error:  1.238407427072525  Accuracy:  0.5477163461538461\n","Epoch:  3  Error:  1.1604523032139509  Accuracy:  0.5821514423076923\n","Epoch:  4  Error:  1.1072061511186453  Accuracy:  0.6032051282051282\n","Epoch:  5  Error:  1.0733148406713438  Accuracy:  0.614803685897436\n","Epoch:  6  Error:  1.0450280990356053  Accuracy:  0.6280849358974359\n","Epoch:  7  Error:  1.0189929687059842  Accuracy:  0.6352363782051282\n","Epoch:  8  Error:  0.997243180947426  Accuracy:  0.6438301282051282\n","Epoch:  9  Error:  0.9766303728788327  Accuracy:  0.6540665064102564\n","Epoch:  10  Error:  0.9616725834516379  Accuracy:  0.6582932692307693\n","Epoch:  11  Error:  0.9454503698226733  Accuracy:  0.6639623397435898\n","Epoch:  12  Error:  0.9336168417563805  Accuracy:  0.669110576923077\n","Epoch:  13  Error:  0.906637488420193  Accuracy:  0.6774038461538462\n","Epoch:  14  Error:  0.8889791945616404  Accuracy:  0.6863982371794872\n","Epoch:  15  Error:  0.8835817648814275  Accuracy:  0.6891426282051282\n","Epoch:  16  Error:  0.862743935523889  Accuracy:  0.6951522435897436\n","Epoch:  17  Error:  0.8616339753835629  Accuracy:  0.6938701923076923\n","Epoch:  18  Error:  0.8390749755578163  Accuracy:  0.7030649038461538\n","Epoch:  19  Error:  0.8337415138880412  Accuracy:  0.7053685897435897\n","Epoch:  20  Error:  0.8147758143070417  Accuracy:  0.7133413461538461\n","Epoch:  21  Error:  0.8057124461883154  Accuracy:  0.7170873397435897\n","Epoch:  22  Error:  0.796050870571381  Accuracy:  0.721854967948718\n","Epoch:  23  Error:  0.7804300609307412  Accuracy:  0.7233774038461539\n","Epoch:  24  Error:  0.7672386210698348  Accuracy:  0.7292267628205128\n","Epoch:  25  Error:  0.7574407164867107  Accuracy:  0.7329927884615385\n","Epoch:  26  Error:  0.7549340981703538  Accuracy:  0.7332131410256411\n","Epoch:  27  Error:  0.738460341325173  Accuracy:  0.7409455128205128\n","Epoch:  28  Error:  0.7089133600393931  Accuracy:  0.7487780448717949\n","Epoch:  29  Error:  0.7007550470339946  Accuracy:  0.7546474358974359\n","Epoch:  30  Error:  0.6870824019114177  Accuracy:  0.7594551282051282\n","Epoch:  31  Error:  0.681804532958911  Accuracy:  0.7625801282051282\n","Epoch:  32  Error:  0.6784676081094987  Accuracy:  0.7636418269230769\n","Epoch:  33  Error:  0.658806736041338  Accuracy:  0.7696714743589743\n","Epoch:  34  Error:  0.6386261474627715  Accuracy:  0.7753205128205128\n","Epoch:  35  Error:  0.6345247982404171  Accuracy:  0.7791466346153846\n","Epoch:  36  Error:  0.6331060641851181  Accuracy:  0.7786858974358974\n","Epoch:  37  Error:  0.619601086469797  Accuracy:  0.7831129807692307\n","Epoch:  38  Error:  0.6090315507772641  Accuracy:  0.7887820512820513\n","Epoch:  39  Error:  0.5980534855371866  Accuracy:  0.7919471153846154\n","Epoch:  40  Error:  0.5831105295664225  Accuracy:  0.8008012820512821\n","Epoch:  41  Error:  0.5772368177389487  Accuracy:  0.8012019230769231\n","Epoch:  42  Error:  0.5672097015075195  Accuracy:  0.8041666666666667\n","Epoch:  43  Error:  0.5575462555273986  Accuracy:  0.8084935897435898\n","Epoch:  44  Error:  0.5417558636420813  Accuracy:  0.8118990384615384\n","Epoch:  45  Error:  0.5384550692179264  Accuracy:  0.8160857371794872\n","Epoch:  46  Error:  0.5301502691629606  Accuracy:  0.8176081730769231\n","Epoch:  47  Error:  0.5234669014429435  Accuracy:  0.8189503205128205\n","Epoch:  48  Error:  0.5249067114713865  Accuracy:  0.8190504807692308\n","Epoch:  49  Error:  0.5190472044241734  Accuracy:  0.8219951923076924\n","Epoch:  50  Error:  0.5010738251300958  Accuracy:  0.8279847756410257\n","Epoch:  51  Error:  0.49125372163760356  Accuracy:  0.8318309294871795\n","Epoch:  52  Error:  0.5058331881578152  Accuracy:  0.826542467948718\n","Epoch:  53  Error:  0.4940953934421906  Accuracy:  0.8321915064102564\n","Epoch:  54  Error:  0.48726363724622973  Accuracy:  0.8344951923076923\n","Epoch:  55  Error:  0.4768664633616423  Accuracy:  0.8390024038461539\n","Epoch:  56  Error:  0.46963190803161037  Accuracy:  0.8419471153846154\n","Epoch:  57  Error:  0.46387929893456975  Accuracy:  0.8422676282051282\n","Epoch:  58  Error:  0.47360088351445323  Accuracy:  0.8387219551282051\n","Epoch:  59  Error:  0.4559623846640954  Accuracy:  0.8454727564102564\n","Epoch:  60  Error:  0.44601028301776985  Accuracy:  0.8490384615384615\n","Epoch:  61  Error:  0.45253352240110056  Accuracy:  0.8474559294871795\n","Epoch:  62  Error:  0.43024230775160666  Accuracy:  0.8557291666666667\n","Epoch:  63  Error:  0.4347351058553427  Accuracy:  0.8544070512820513\n","Epoch:  64  Error:  0.42172542000428226  Accuracy:  0.8568709935897436\n","Epoch:  65  Error:  0.44512734321447517  Accuracy:  0.8501802884615385\n","Epoch:  66  Error:  0.4080310747027397  Accuracy:  0.8635016025641026\n","Epoch:  67  Error:  0.4161280801663032  Accuracy:  0.8601963141025641\n","Epoch:  68  Error:  0.394622025008385  Accuracy:  0.867568108974359\n","Epoch:  69  Error:  0.42749692018215474  Accuracy:  0.8579126602564102\n","Epoch:  70  Error:  0.4051941596162625  Accuracy:  0.8660857371794872\n","Epoch:  71  Error:  0.3949237246926014  Accuracy:  0.8677083333333333\n","Epoch:  72  Error:  0.4065069561203321  Accuracy:  0.8664863782051282\n","Epoch:  73  Error:  0.38566930446869285  Accuracy:  0.87109375\n","Epoch:  74  Error:  0.3962967785887229  Accuracy:  0.8680288461538461\n","Epoch:  75  Error:  0.3863432471186687  Accuracy:  0.8717748397435897\n","Epoch:  76  Error:  0.3808893907910738  Accuracy:  0.8747596153846153\n","Epoch:  77  Error:  0.3770852492405818  Accuracy:  0.873417467948718\n","Epoch:  78  Error:  0.38123110013130385  Accuracy:  0.8754407051282052\n","Epoch:  79  Error:  0.3918188071021667  Accuracy:  0.8711738782051283\n","Epoch:  80  Error:  0.3637011889845897  Accuracy:  0.880048076923077\n","Epoch:  81  Error:  0.38073414407479456  Accuracy:  0.8740184294871794\n","Epoch:  82  Error:  0.3529557475676903  Accuracy:  0.8838541666666667\n","Epoch:  83  Error:  0.358144764143687  Accuracy:  0.8816105769230769\n","Epoch:  84  Error:  0.3564734568580603  Accuracy:  0.88359375\n","Epoch:  85  Error:  0.36597550335602885  Accuracy:  0.8783052884615384\n","Epoch:  86  Error:  0.36541931105729863  Accuracy:  0.8810697115384616\n","Epoch:  87  Error:  0.3656051702606372  Accuracy:  0.88046875\n","Epoch:  88  Error:  0.3609372487817055  Accuracy:  0.8799679487179487\n","Epoch:  89  Error:  0.34461074941433395  Accuracy:  0.8871394230769231\n","Epoch:  90  Error:  0.3523518912685223  Accuracy:  0.8857972756410256\n","Epoch:  91  Error:  0.3434756905604631  Accuracy:  0.8883613782051282\n","Epoch:  92  Error:  0.34068798415171797  Accuracy:  0.8899238782051282\n","Epoch:  93  Error:  0.3455251123278569  Accuracy:  0.8874399038461539\n","Epoch:  94  Error:  0.33932365018587846  Accuracy:  0.8892828525641026\n","Epoch:  95  Error:  0.33071855360116714  Accuracy:  0.8929286858974359\n","Epoch:  96  Error:  0.3490360273000522  Accuracy:  0.8856169871794872\n","Epoch:  97  Error:  0.3379256797524599  Accuracy:  0.8906049679487179\n","Epoch:  98  Error:  0.3404667032070649  Accuracy:  0.8891626602564102\n","Epoch:  99  Error:  0.37316364023165827  Accuracy:  0.8795472756410256\n","Final results:\n","  test loss:\t\t\t1.897584\n","  test accuracy:\t\t59.11 %\n"],"name":"stdout"}]},{"metadata":{"id":"VHuUy6I42pPh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"af3c4f65-0c9c-44db-dfb9-9ceb0e423484","executionInfo":{"status":"ok","timestamp":1534341299879,"user_tz":-540,"elapsed":2050,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["!ls cifar-10-batches-py\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["batches.meta  data_batch_2  data_batch_4  readme.html\r\n","data_batch_1  data_batch_3  data_batch_5  test_batch\r\n"],"name":"stdout"}]},{"metadata":{"id":"y8t01ph32oGr","colab_type":"text"},"cell_type":"markdown","source":[""]}]}