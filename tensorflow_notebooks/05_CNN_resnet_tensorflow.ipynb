{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_CNN_resnet_tensorflow.ipynb","version":"0.3.2","provenance":[{"file_id":"1bckWr4axkGwK4FeNNw75Td5pIEiCIM_O","timestamp":1534593898245},{"file_id":"1dDWmWqRvB8y6d0ePFOJVfWVeeXz63T9F","timestamp":1534558134470},{"file_id":"1d0ZZZxxFS1QUfMtCZC_wTMOKkmvwRtSM","timestamp":1534333622972},{"file_id":"1crLbZF0eepPiNP9yhc4HcO_LBPQ1_jy4","timestamp":1534325989751}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"6KXGk-IwxKyr","colab_type":"text"},"cell_type":"markdown","source":["#TensorFlowによる畳み込みニューラルネットワーク(CIFAR10)"]},{"metadata":{"id":"wiXv2wIayBIp","colab_type":"text"},"cell_type":"markdown","source":["TensorFlowでニューラルネットワークのプログラムを実装する方法として，セッションを準備して実行する方法，Estimatorを利用する方法，Eagerを利用する方法があります．ここでは，まずセッションを準備する方法について説明します．また，学習および評価データセットとしてCIFAR10を用います．CIFAR10は，10クラスの一般物体認識のデータセットです．高い認識精度を達成するためには，データ拡張(data augmentation)が重要です．ここでは，データ拡張の処理を追加します．"]},{"metadata":{"id":"rJF5Dboz0vdb","colab_type":"text"},"cell_type":"markdown","source":["必要なパッケージをインポートします．\n","また，GPUが利用可能かを確認します．"]},{"metadata":{"id":"p3TDtyQgxApb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b98fe12f-2509-4e18-a021-24d7a3e79b2e","executionInfo":{"status":"ok","timestamp":1534806101870,"user_tz":-540,"elapsed":9865,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["import os\n","import sys\n","import tarfile\n","from six.moves import urllib\n","import numpy as np\n","import pickle\n","import glob\n","import tensorflow as tf\n","\n","tf.test.gpu_device_name()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"YzZcm4xOyCJ9","colab_type":"text"},"cell_type":"markdown","source":["CIFARデータセットをダウンロードします．ここでは，pythonでデータを扱うためにpython用に用意されたデータセットを利用します．"]},{"metadata":{"id":"W9IP2sbRx-K9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"31624f18-46f7-446e-8755-9cd15fe89732","executionInfo":{"status":"ok","timestamp":1534806167035,"user_tz":-540,"elapsed":63987,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["dest_directory = \"./\"\n","DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n","\n","if not os.path.exists(dest_directory):\n","    os.makedirs(dest_directory)\n","\n","filename = DATA_URL.split('/')[-1]\n","filepath = os.path.join(dest_directory, filename)\n","\n","if not os.path.exists(filepath):\n","    def _progress(count, block_size, total_size):\n","      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,  float(count * block_size) / float(total_size) * 100.0))\n","      sys.stdout.flush()\n","    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n","    print()\n","    statinfo = os.stat(filepath)\n","    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n","\n","extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-py')\n","if not os.path.exists(extracted_dir_path):\n","  tarfile.open(filepath, 'r:gz').extractall(dest_directory)"],"execution_count":2,"outputs":[{"output_type":"stream","text":[">> Downloading cifar-10-python.tar.gz 100.0%\n","Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n"],"name":"stdout"}]},{"metadata":{"id":"eN6jzxk144ZW","colab_type":"text"},"cell_type":"markdown","source":["ダウンロードしたCIFAR10データセットを読み込みます．ダウンロードしたデータは学習用に5つ，評価用に１つのファイルがあり，バイト形式になっています．これをnumpy形式のデータに変換します．各画像は32x32ピクセルのカラー画像です．よって32x32=1024バイトごとデータをチャンネルとして重ね合わせることでカラー画像にすることができます．教師データは0から9のクラスラベルです．学習データとその教師ラベルをそれぞれX_train，y_trainとします．また，評価データとその教師ラベルをX_test，y_testとします．\n","\n","そして，各データを平均０，分散１になるように正規化を行います．\n","これは，画像のばらつきを抑える前処理として利用されている方法です．"]},{"metadata":{"id":"q0afEE2CjItx","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_dataset(dirpath='cifar-10-batches-py'):\n","    X, y = [], []\n","    # take data from the data batch\n","    for path in glob.glob('%s/data_batch_*' % dirpath):\n","        with open(path, 'rb') as f:\n","            batch = pickle.load(f, encoding='bytes')\n","        # append all data and labels from the 5 data betch\n","        X.append(batch[b'data'])\n","        y.append(batch[b'labels'])\n","    # devide by 255 for making value 0 to 1\n","    X = np.concatenate(X) /np.float32(255)\n","    # making labels as int\n","    y = np.concatenate(y).astype(np.int64)\n","    #seperate in to RGB colors\n","    X = np.dstack((X[:, :1024], X[:, 1024:2048], X[:, 2048:]))\n","    # reshape data into 4D tensor with compatible to CNN model\n","    X_train = X.reshape((X.shape[0], 32, 32, 3))\n","    y_train = y.reshape((y.shape[0]))\n","        \n","    # load test set\n","    path = '%s/test_batch' % dirpath\n","    with open(path, 'rb') as f:\n","        batch = pickle.load(f, encoding='bytes')\n","    X_test = batch[b'data'] /np.float32(255)\n","    X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:]))\n","    X_test = X_test.reshape((X_test.shape[0], 32, 32, 3))\n","    y_t = np.array(batch[b'labels'], dtype=np.int64)\n","    y_test = y_t.reshape((y_t.shape[0]))\n","\n","    # normalize to zero mean and unity variance\n","    offset = np.mean(X_train, 0)\n","    scale = np.std(X_train, 0).clip(min=1)\n","    #scale =1\n","    X_train = (X_train - offset) / scale\n","    X_test = (X_test - offset) / scale\n","    \n","    \n","    \n","    return X_train, y_train, X_test, y_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e698krXc3fo6","colab_type":"text"},"cell_type":"markdown","source":["ネットワークモデルをcnnとして定義します．入力(input_val)は，32x32x3というカラー画像とします．１層目の畳み込み層のフィルタは，x['w1']とし，具体的なフィルタサイズとフィルタ数はパラメータの初期化時に決めた値とします．畳み込み演算を行うストライドは[1,1,1,1]とし，ゼロパディングを行う設定(padding='SAME')としています．そして，バイアス(b['b'])の値を加算し，活性化関数reluを適用した値をconv1とします．\n","次に，最大値プーリングを行います．\n","２層目以降の畳み込みも同様に設定を行います．\n","全結層のユニット数もパラメータの初期化時に決めた値とします．\n","最後にソフトマックス関数を適用して各クラスの確率を求めます．\n","戻り値は，全結合層の値とクラス確率とします．\n"]},{"metadata":{"id":"DZHwz5ZddCKG","colab_type":"code","colab":{}},"cell_type":"code","source":["def weight_variable(shape, name=None):\n","    initial = tf.truncated_normal(shape, stddev=0.1)\n","    return tf.Variable(initial, name=name)\n","\n","def softmax_layer(inpt, shape):\n","    fc_w = weight_variable(shape)\n","    fc_b = tf.Variable(tf.zeros([shape[1]]))\n","\n","    fc_h = tf.nn.softmax(tf.matmul(inpt, fc_w) + fc_b)\n","\n","    return fc_h\n","\n","def conv_layer(inpt, filter_shape, stride):\n","    out_channels = filter_shape[3]\n","\n","    filter_ = weight_variable(filter_shape)\n","    conv = tf.nn.conv2d(inpt, filter=filter_, strides=[1, stride, stride, 1], padding=\"SAME\")\n","    mean, var = tf.nn.moments(conv, axes=[0,1,2])\n","    beta = tf.Variable(tf.zeros([out_channels]), name=\"beta\")\n","    gamma = weight_variable([out_channels], name=\"gamma\")\n","    \n","    batch_norm = tf.nn.batch_norm_with_global_normalization(\n","        conv, mean, var, beta, gamma, 0.001,\n","        scale_after_normalization=True)\n","\n","    out = tf.nn.relu(batch_norm)\n","\n","    return out\n","\n","def residual_block(inpt, output_depth, down_sample, projection=False):\n","    input_depth = inpt.get_shape().as_list()[3]\n","    if down_sample:\n","        filter_ = [1,2,2,1]\n","        inpt = tf.nn.max_pool(inpt, ksize=filter_, strides=filter_, padding='SAME')\n","\n","    conv1 = conv_layer(inpt, [3, 3, input_depth, output_depth], 1)\n","    conv2 = conv_layer(conv1, [3, 3, output_depth, output_depth], 1)\n","\n","    if input_depth != output_depth:\n","        if projection:\n","            # Option B: Projection shortcut\n","            input_layer = conv_layer(inpt, [1, 1, input_depth, output_depth], 2)\n","        else:\n","            # Option A: Zero-padding\n","            input_layer = tf.pad(inpt, [[0,0], [0,0], [0,0], [0, output_depth - input_depth]])\n","    else:\n","        input_layer = inpt\n","\n","    res = conv2 + input_layer\n","    return res\n","\n","n_dict = {20:1, 32:2, 44:3, 56:4}\n","\n","def resnet(inpt, n):\n","    if n < 20 or (n - 20) % 12 != 0:\n","        print( \"ResNet depth invalid.\")\n","        return\n","\n","    num_conv = int((n - 20) / 12 + 1)\n","    layers = []\n","\n","    with tf.variable_scope('conv1'):\n","        conv1 = conv_layer(inpt, [3, 3, 3, 16], 1)\n","        layers.append(conv1)\n","\n","    for i in range (num_conv):\n","        with tf.variable_scope('conv2_%d' % (i+1)):\n","            conv2_x = residual_block(layers[-1], 16, False)\n","            conv2 = residual_block(conv2_x, 16, False)\n","            layers.append(conv2_x)\n","            layers.append(conv2)\n","\n","        assert conv2.get_shape().as_list()[1:] == [32, 32, 16]\n","\n","    for i in range (num_conv):\n","        down_sample = True if i == 0 else False\n","        with tf.variable_scope('conv3_%d' % (i+1)):\n","            conv3_x = residual_block(layers[-1], 32, down_sample)\n","            conv3 = residual_block(conv3_x, 32, False)\n","            layers.append(conv3_x)\n","            layers.append(conv3)\n","\n","        assert conv3.get_shape().as_list()[1:] == [16, 16, 32]\n","    \n","    for i in range (num_conv):\n","        down_sample = True if i == 0 else False\n","        with tf.variable_scope('conv4_%d' % (i+1)):\n","            conv4_x = residual_block(layers[-1], 64, down_sample)\n","            conv4 = residual_block(conv4_x, 64, False)\n","            layers.append(conv4_x)\n","            layers.append(conv4)\n","\n","        assert conv4.get_shape().as_list()[1:] == [8, 8, 64]\n","\n","    with tf.variable_scope('fc'):\n","        global_pool = tf.reduce_mean(layers[-1], [1, 2])\n","        assert global_pool.get_shape().as_list()[1:] == [64]\n","        \n","        out = softmax_layer(global_pool, [64, 10])\n","        layers.append(out)\n","\n","    return layers[-1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r-9z61EF3shh","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","_BATCH_NORM_DECAY = 0.997\n","_BATCH_NORM_EPSILON = 1e-5\n","DEFAULT_VERSION = 2\n","DEFAULT_DTYPE = tf.float32\n","CASTABLE_TYPES = (tf.float16,)\n","ALLOWED_TYPES = (DEFAULT_DTYPE,) + CASTABLE_TYPES\n","\n","\n","def batch_norm(inputs, training):\n","  return tf.layers.batch_normalization( inputs=inputs, axis=3,\n","                                       momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, \n","                                       center=True,  scale=True, training=training, fused=True)\n","\n","\n","def fixed_padding(inputs, kernel_size):\n","  pad_total = kernel_size - 1\n","  pad_beg = pad_total // 2\n","  pad_end = pad_total - pad_beg\n","\n","  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],  [pad_beg, pad_end], [0, 0]])\n","  return padded_inputs\n","\n","\n","def conv2d_fixed_padding(inputs, filters, kernel_size, strides):\n","  if strides > 1:\n","    inputs = fixed_padding(inputs, kernel_size)\n","\n","  return tf.layers.conv2d(\n","      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n","      padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n","      kernel_initializer=tf.variance_scaling_initializer() )\n","\n","\n","def building_block(inputs, filters, training, projection_shortcut, strides):\n","  shortcut = inputs\n","\n","  if projection_shortcut is not None:\n","    shortcut = projection_shortcut(inputs)\n","    shortcut = batch_norm(shortcut, training)\n","    \n","  inputs = conv2d_fixed_padding( inputs=inputs, filters=filters, kernel_size=3, strides=strides)\n","  inputs = batch_norm(inputs, training)\n","  inputs = tf.nn.relu(inputs)\n","\n","  inputs = conv2d_fixed_padding( inputs=inputs, filters=filters, kernel_size=3, strides=1)\n","  inputs = batch_norm(inputs, training)\n","  inputs += shortcut\n","  inputs = tf.nn.relu(inputs)\n","\n","  return inputs\n","\n","\n","def block_layer(inputs, filters, blocks, strides, training, name):\n","  filters_out = filters\n","\n","  def projection_shortcut(inputs):\n","    return conv2d_fixed_padding(inputs=inputs, filters=filters_out, kernel_size=1, strides=strides)\n","\n","  inputs = building_block(inputs, filters, training, projection_shortcut, strides)\n","\n","  for _ in range(1, blocks):\n","    inputs = building_block(inputs, filters, training, None, 1)\n","\n","  return tf.identity(inputs, name)\n","\n","\n","def resnet(inpt, resnet_size, training):  \n","    num_blocks = (resnet_size - 2) // 6\n","    bottleneck=False\n","    num_classes=10\n","    num_filters=16\n","    kernel_size=3\n","    conv_stride=1\n","    block_sizes=[num_blocks] * 3\n","    block_strides=[1, 2, 2]\n","    final_size=64\n","  \n","    inputs = conv2d_fixed_padding( inpt, num_filters,  kernel_size, conv_stride)\n","    inputs = tf.identity(inputs, 'initial_conv')\n","\n","    inputs = batch_norm(inputs, training)\n","    inputs = tf.nn.relu(inputs)\n","\n","    for i, num_blocks in enumerate(block_sizes):\n","      inputs = block_layer(inputs, num_filters * (2**i), num_blocks, \n","                           block_strides[i], training, 'block_layer{}'.format(i + 1))\n","\n","\n","    inputs = tf.reduce_mean(inputs, [1, 2], keepdims=True)\n","    inputs = tf.identity(inputs, 'final_reduce_mean')\n","\n","    inputs = tf.reshape(inputs, [-1, final_size])\n","    inputs = tf.layers.dense(inputs=inputs, units=num_classes)\n","    inputs = tf.identity(inputs, 'final_dense')\n","    \n","    return inputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Q39mHLvyk4Y","colab_type":"text"},"cell_type":"markdown","source":["学習データと教師ラベルを準備します．\n","placeholderは，データを格納する入れ物です．ここでは，入れ物のサイズだけを決めて，具体的な値は実行する時に与えます．"]},{"metadata":{"id":"FI879-osyihG","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder(tf.float32,[None,32,32,3])\n","y = tf.placeholder(tf.int64,[None])\n","learning_rate = tf.placeholder(tf.float32,[])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IhMF1nwZ4Tb6","colab_type":"text"},"cell_type":"markdown","source":["ネットワークモデルを定義します．"]},{"metadata":{"id":"5kO6rXPY4Z5U","colab_type":"code","colab":{}},"cell_type":"code","source":["predict= resnet(x,20)\n","#predict= resnet(x,20, True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xgcL-8mu4dtB","colab_type":"text"},"cell_type":"markdown","source":["誤差関数と最適化の方法を定義します．ここでは，物体認識を行うのでソフトマックスクロスエントロピーを誤差関数とします．最適化の方法には，Adamを利用します．誤差はminimizeで最小化するようにします．"]},{"metadata":{"id":"8atjy0aq5c2T","colab_type":"code","colab":{}},"cell_type":"code","source":["error = tf.reduce_mean( tf.losses.sparse_softmax_cross_entropy(logits = predict,labels = y))\n","optm =tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(error)\n","\n","\n","#cross_entropy_mean =  tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits = predict,labels = y) )\n","#regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","#error = tf.add_n([cross_entropy_mean] + regularization_losses)\n","#optm =tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(error)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zza5fkvU5d8-","colab_type":"text"},"cell_type":"markdown","source":["次に評価を行う準備をします．ネットワークからの出力値が最大となるクラスの番号と教師データが一致するかをequalで判定します．全てのデータに対する結果をまとめてaccuracyを算出します．"]},{"metadata":{"id":"llC2SFfm52E5","colab_type":"code","colab":{}},"cell_type":"code","source":["corr = tf.equal(tf.argmax(predict, 1), y)\n","accuracy = tf.reduce_mean(tf.cast(corr, tf.float32)) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"O9z66BJ37bmj","colab_type":"text"},"cell_type":"markdown","source":["ミニバッチ学習として複数の学習データを渡すための関数iterate_minibatchesを用意します．ここでは，学習データのインデックス番号をシャッフルし，その順番に学習データとその教師データをミニバッチサイズ(batchsize)分戻り値とします．呼び出されるたびに順次異なるデータを渡していきます．"]},{"metadata":{"id":"_8T7o917nsdY","colab_type":"code","colab":{}},"cell_type":"code","source":["def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n","    assert len(inputs) == len(targets)\n","    if shuffle:\n","        indices = np.arange(len(inputs))\n","        np.random.shuffle(indices)\n","    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n","        if shuffle:\n","            excerpt = indices[start_idx:start_idx + batchsize]\n","        else:\n","            excerpt = slice(start_idx, start_idx + batchsize)\n","        yield inputs[excerpt], targets[excerpt]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gas1-BurzYWa","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"OTzr3amLzXoB","colab_type":"code","colab":{}},"cell_type":"code","source":["from scipy.misc import imresize\n","\n","\n","def data_augmentation(inputs, batchsize):\n","\n","  aug_inputs = np.zeros((inputs.shape[0], inputs.shape[1], inputs.shape[2], inputs.shape[3]),dtype = np.float32)\n","  for idx in range(0, len(inputs)):\n","      crop_size = np.random.randint(28, 32)\n","      aug_inputs[idx] = np.copy(inputs[idx])\n","      h, w, _ = inputs[idx].shape\n","      if np.random.rand() < 0.5:\n","        aug_inputs[idx] = aug_inputs[idx][:, ::-1, :]\n","      \n","      if np.random.rand() < 0.5:\n","          top = np.random.randint(0, h - crop_size)\n","          left = np.random.randint(0, w - crop_size)\n","          bottom = top + crop_size\n","          right = left + crop_size\n","          scale_img = aug_inputs[idx][top:bottom, left:right, :]\n","          aug_inputs[idx] = imresize(scale_img, (32, 32))\n","\n","          \n","  return aug_inputs\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"eZdpwC-861xK","colab_type":"text"},"cell_type":"markdown","source":["学習を実行するために，セッションをtf.Session()で準備します．そして，セッションをsess.runで実行しますが，最初にネットワークのパラメータの初期化を行います．for文内で指定したエポック数分の学習を行います．そして，iterate_minibatchesで取得したミニバッチ分の学習データを利用して学習を行います．最初のsess.run(optm, feed_dict・・・)で，fee_dictとして与えたデータをネットワークに順伝播し，誤差を算出します．そして，sess.run([error, accuracy], feed_dict・・・)で，誤差と精度を算出します．\n","１エポック分の学習が終わると，誤差と精度を画面に表示します．"]},{"metadata":{"id":"XJ0Auo2Gu2TU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3114},"outputId":"4dee029e-ed9e-4001-d790-46c810e678cb"},"cell_type":"code","source":["num_epochs = 300\n","max_accuracy = 0\n","max_epoch = 0\n","rate = 0.1\n","weight_decay = 2e-4\n","\n","out_train = open(\"log_train.txt\",'w')\n","out_test = open(\"log_test.txt\",'w')\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    print(\"loading dataset...\")\n","    X_train,y_train, X_test,y_test = load_dataset()\n","    print(\"Starting training...\")\n","    \n","    for epoch in range(num_epochs):\n","        train_err = 0\n","        train_acc = 0\n","        train_batches = 0\n","        if epoch == 100: rate = 0.01\n","        if epoch == 150: rate = 0.001\n","        if epoch == 200: rate = 0.0001\n","        \n","        for batch in iterate_minibatches(X_train, y_train, 128, shuffle=True):\n","            aug_inputs = data_augmentation(batch[0], 128)\n","            tt = sess.run(optm,feed_dict = {x: aug_inputs,y: batch[1], learning_rate:rate})\n","#            tt = sess.run(optm,feed_dict = {x: batch[0],y: batch[1]})\n","            err,acc= sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})\n","            \n","            train_err += err\n","            train_acc += acc\n","            train_batches += 1\n","        print(\"  Epoch: {} loss:{:6f}\\tAccuracy:{:.2f}%\\t lerning rate:{}\".format(epoch+1, train_err / train_batches, train_acc/train_batches * 100, rate))\n","#        print(\"Epoch: \",  epoch+1, \" Error: \", train_err/train_batches, \" Accuracy: \", train_acc/train_batches )    \n","        out_train.write(\"{}\\t{:.6f}\\t{:2f}\\n\".format(epoch+1,train_err/train_batches,  train_acc/train_batches * 100))\n","    \n","        if (epoch+1) %10 ==0:\n","            test_err = 0\n","            test_acc = 0\n","            test_batches = 0\n","            for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n","                err, acc = sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})# apply tensor function\n","                test_err += err\n","                test_acc += acc\n","                test_batches += 1\n","            cur_acc = test_acc / test_batches * 100\n","            print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n","            print(\"  test accuracy:\\t\\t{:.2f} %\".format(cur_acc))\n","            out_test.write(\"{}\\t{:.6f}\\t{:2f}\\n\".format(epoch+1,test_err / test_batches,  cur_acc))\n","\n","            if max_accuracy < cur_acc:\n","                max_accuracy = cur_acc\n","                max_epoch = epoch+1\n","print(\"Best epoch: {}\".format(max_epoch))\n","print(\"Best accuracy:\\t\\t{:.2f} %\".format(max_accuracy))\n","out_train.close()\n","out_test.close()\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading dataset...\n","Starting training...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if issubdtype(ts, int):\n","/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n","  elif issubdtype(type(size), float):\n"],"name":"stderr"},{"output_type":"stream","text":["  Epoch: 1 loss:2.262304\tAccuracy:18.09%\t lerning rate:0.1\n","  Epoch: 2 loss:2.196288\tAccuracy:26.31%\t lerning rate:0.1\n","  Epoch: 3 loss:2.133001\tAccuracy:33.31%\t lerning rate:0.1\n","  Epoch: 4 loss:2.071290\tAccuracy:39.63%\t lerning rate:0.1\n","  Epoch: 5 loss:2.022983\tAccuracy:44.48%\t lerning rate:0.1\n","  Epoch: 6 loss:1.995273\tAccuracy:47.20%\t lerning rate:0.1\n","  Epoch: 7 loss:1.974138\tAccuracy:49.43%\t lerning rate:0.1\n","  Epoch: 8 loss:1.950053\tAccuracy:51.90%\t lerning rate:0.1\n","  Epoch: 9 loss:1.940758\tAccuracy:52.58%\t lerning rate:0.1\n","  Epoch: 10 loss:1.926965\tAccuracy:54.07%\t lerning rate:0.1\n","  test loss:\t\t\t1.946956\n","  test accuracy:\t\t52.08 %\n","  Epoch: 11 loss:1.917419\tAccuracy:54.81%\t lerning rate:0.1\n","  Epoch: 12 loss:1.903905\tAccuracy:56.26%\t lerning rate:0.1\n","  Epoch: 13 loss:1.894637\tAccuracy:57.19%\t lerning rate:0.1\n","  Epoch: 14 loss:1.897907\tAccuracy:56.51%\t lerning rate:0.1\n","  Epoch: 15 loss:1.888075\tAccuracy:57.58%\t lerning rate:0.1\n","  Epoch: 16 loss:1.879869\tAccuracy:58.28%\t lerning rate:0.1\n","  Epoch: 17 loss:1.873187\tAccuracy:59.06%\t lerning rate:0.1\n","  Epoch: 18 loss:1.867944\tAccuracy:59.45%\t lerning rate:0.1\n","  Epoch: 19 loss:1.862928\tAccuracy:60.02%\t lerning rate:0.1\n","  Epoch: 20 loss:1.860957\tAccuracy:60.33%\t lerning rate:0.1\n","  test loss:\t\t\t1.871756\n","  test accuracy:\t\t59.04 %\n","  Epoch: 21 loss:1.854953\tAccuracy:60.74%\t lerning rate:0.1\n","  Epoch: 22 loss:1.851626\tAccuracy:61.07%\t lerning rate:0.1\n","  Epoch: 23 loss:1.855522\tAccuracy:60.75%\t lerning rate:0.1\n","  Epoch: 24 loss:1.849692\tAccuracy:61.44%\t lerning rate:0.1\n","  Epoch: 25 loss:1.843194\tAccuracy:61.90%\t lerning rate:0.1\n","  Epoch: 26 loss:1.841795\tAccuracy:62.05%\t lerning rate:0.1\n","  Epoch: 27 loss:1.842683\tAccuracy:61.92%\t lerning rate:0.1\n","  Epoch: 28 loss:1.837063\tAccuracy:62.58%\t lerning rate:0.1\n","  Epoch: 29 loss:1.828800\tAccuracy:63.39%\t lerning rate:0.1\n","  Epoch: 30 loss:1.824242\tAccuracy:63.94%\t lerning rate:0.1\n","  test loss:\t\t\t1.813207\n","  test accuracy:\t\t65.13 %\n","  Epoch: 31 loss:1.819871\tAccuracy:64.23%\t lerning rate:0.1\n","  Epoch: 32 loss:1.815709\tAccuracy:64.70%\t lerning rate:0.1\n","  Epoch: 33 loss:1.814127\tAccuracy:64.74%\t lerning rate:0.1\n","  Epoch: 34 loss:1.815248\tAccuracy:64.64%\t lerning rate:0.1\n","  Epoch: 35 loss:1.811396\tAccuracy:65.07%\t lerning rate:0.1\n","  Epoch: 36 loss:1.812390\tAccuracy:64.98%\t lerning rate:0.1\n","  Epoch: 37 loss:1.802894\tAccuracy:65.92%\t lerning rate:0.1\n","  Epoch: 38 loss:1.798240\tAccuracy:66.36%\t lerning rate:0.1\n","  Epoch: 39 loss:1.806480\tAccuracy:65.41%\t lerning rate:0.1\n","  Epoch: 40 loss:1.813129\tAccuracy:64.71%\t lerning rate:0.1\n","  test loss:\t\t\t1.827349\n","  test accuracy:\t\t63.31 %\n","  Epoch: 41 loss:1.835190\tAccuracy:62.41%\t lerning rate:0.1\n","  Epoch: 42 loss:1.847460\tAccuracy:61.27%\t lerning rate:0.1\n","  Epoch: 43 loss:1.870686\tAccuracy:58.79%\t lerning rate:0.1\n","  Epoch: 44 loss:1.873156\tAccuracy:58.48%\t lerning rate:0.1\n","  Epoch: 45 loss:1.887280\tAccuracy:57.08%\t lerning rate:0.1\n","  Epoch: 46 loss:1.955356\tAccuracy:50.08%\t lerning rate:0.1\n","  Epoch: 47 loss:1.934866\tAccuracy:52.29%\t lerning rate:0.1\n","  Epoch: 48 loss:1.928573\tAccuracy:53.02%\t lerning rate:0.1\n","  Epoch: 49 loss:1.924053\tAccuracy:53.38%\t lerning rate:0.1\n","  Epoch: 50 loss:1.944747\tAccuracy:51.28%\t lerning rate:0.1\n","  test loss:\t\t\t1.994759\n","  test accuracy:\t\t46.20 %\n","  Epoch: 51 loss:1.959262\tAccuracy:49.80%\t lerning rate:0.1\n","  Epoch: 52 loss:1.965857\tAccuracy:49.15%\t lerning rate:0.1\n","  Epoch: 53 loss:1.972592\tAccuracy:48.49%\t lerning rate:0.1\n","  Epoch: 54 loss:1.980691\tAccuracy:47.61%\t lerning rate:0.1\n","  Epoch: 55 loss:1.962704\tAccuracy:49.49%\t lerning rate:0.1\n","  Epoch: 56 loss:1.946169\tAccuracy:51.17%\t lerning rate:0.1\n","  Epoch: 57 loss:1.957107\tAccuracy:50.01%\t lerning rate:0.1\n","  Epoch: 58 loss:1.950283\tAccuracy:50.61%\t lerning rate:0.1\n","  Epoch: 59 loss:1.953712\tAccuracy:50.35%\t lerning rate:0.1\n","  Epoch: 60 loss:1.945852\tAccuracy:51.14%\t lerning rate:0.1\n","  test loss:\t\t\t1.960066\n","  test accuracy:\t\t49.59 %\n","  Epoch: 61 loss:1.951163\tAccuracy:50.63%\t lerning rate:0.1\n","  Epoch: 62 loss:1.949929\tAccuracy:50.70%\t lerning rate:0.1\n","  Epoch: 63 loss:1.961051\tAccuracy:49.64%\t lerning rate:0.1\n","  Epoch: 64 loss:1.970503\tAccuracy:48.72%\t lerning rate:0.1\n","  Epoch: 65 loss:1.975954\tAccuracy:48.04%\t lerning rate:0.1\n","  Epoch: 66 loss:1.967750\tAccuracy:48.93%\t lerning rate:0.1\n","  Epoch: 67 loss:1.943911\tAccuracy:51.34%\t lerning rate:0.1\n","  Epoch: 68 loss:1.969443\tAccuracy:48.79%\t lerning rate:0.1\n","  Epoch: 69 loss:1.966133\tAccuracy:49.14%\t lerning rate:0.1\n","  Epoch: 70 loss:1.951863\tAccuracy:50.46%\t lerning rate:0.1\n","  test loss:\t\t\t1.967284\n","  test accuracy:\t\t49.05 %\n","  Epoch: 71 loss:1.967285\tAccuracy:49.03%\t lerning rate:0.1\n","  Epoch: 72 loss:1.962282\tAccuracy:49.54%\t lerning rate:0.1\n","  Epoch: 73 loss:1.955545\tAccuracy:50.19%\t lerning rate:0.1\n","  Epoch: 74 loss:1.965350\tAccuracy:49.20%\t lerning rate:0.1\n","  Epoch: 75 loss:1.977105\tAccuracy:48.00%\t lerning rate:0.1\n","  Epoch: 76 loss:1.969004\tAccuracy:48.80%\t lerning rate:0.1\n","  Epoch: 77 loss:1.971479\tAccuracy:48.53%\t lerning rate:0.1\n","  Epoch: 78 loss:1.960897\tAccuracy:49.58%\t lerning rate:0.1\n","  Epoch: 79 loss:1.955868\tAccuracy:50.13%\t lerning rate:0.1\n","  Epoch: 80 loss:1.956698\tAccuracy:50.03%\t lerning rate:0.1\n","  test loss:\t\t\t1.959129\n","  test accuracy:\t\t49.74 %\n","  Epoch: 81 loss:1.959550\tAccuracy:49.73%\t lerning rate:0.1\n","  Epoch: 82 loss:1.965445\tAccuracy:49.21%\t lerning rate:0.1\n","  Epoch: 83 loss:1.968723\tAccuracy:48.82%\t lerning rate:0.1\n","  Epoch: 84 loss:1.969056\tAccuracy:48.82%\t lerning rate:0.1\n","  Epoch: 85 loss:1.969501\tAccuracy:48.72%\t lerning rate:0.1\n","  Epoch: 86 loss:1.954849\tAccuracy:50.23%\t lerning rate:0.1\n","  Epoch: 87 loss:1.966657\tAccuracy:49.01%\t lerning rate:0.1\n","  Epoch: 88 loss:1.969788\tAccuracy:48.72%\t lerning rate:0.1\n","  Epoch: 89 loss:1.969246\tAccuracy:48.80%\t lerning rate:0.1\n","  Epoch: 90 loss:2.009836\tAccuracy:44.71%\t lerning rate:0.1\n","  test loss:\t\t\t2.038059\n","  test accuracy:\t\t41.82 %\n","  Epoch: 91 loss:2.012143\tAccuracy:44.51%\t lerning rate:0.1\n","  Epoch: 92 loss:1.992224\tAccuracy:46.53%\t lerning rate:0.1\n","  Epoch: 93 loss:2.005431\tAccuracy:45.10%\t lerning rate:0.1\n","  Epoch: 94 loss:1.978043\tAccuracy:47.95%\t lerning rate:0.1\n","  Epoch: 95 loss:1.990014\tAccuracy:46.73%\t lerning rate:0.1\n","  Epoch: 96 loss:1.987525\tAccuracy:47.07%\t lerning rate:0.1\n","  Epoch: 97 loss:2.000067\tAccuracy:45.72%\t lerning rate:0.1\n","  Epoch: 98 loss:1.990328\tAccuracy:46.68%\t lerning rate:0.1\n","  Epoch: 99 loss:1.972652\tAccuracy:48.53%\t lerning rate:0.1\n","  Epoch: 100 loss:1.974157\tAccuracy:48.37%\t lerning rate:0.1\n","  test loss:\t\t\t1.998502\n","  test accuracy:\t\t45.80 %\n","  Epoch: 101 loss:1.971180\tAccuracy:48.61%\t lerning rate:0.01\n","  Epoch: 102 loss:1.967188\tAccuracy:48.98%\t lerning rate:0.01\n","  Epoch: 103 loss:1.961103\tAccuracy:49.67%\t lerning rate:0.01\n","  Epoch: 104 loss:1.961245\tAccuracy:49.63%\t lerning rate:0.01\n","  Epoch: 105 loss:1.967434\tAccuracy:48.97%\t lerning rate:0.01\n","  Epoch: 106 loss:1.968837\tAccuracy:48.84%\t lerning rate:0.01\n","  Epoch: 107 loss:1.965598\tAccuracy:49.18%\t lerning rate:0.01\n","  Epoch: 108 loss:1.965461\tAccuracy:49.20%\t lerning rate:0.01\n","  Epoch: 109 loss:1.967264\tAccuracy:49.10%\t lerning rate:0.01\n","  Epoch: 110 loss:1.967154\tAccuracy:48.98%\t lerning rate:0.01\n","  test loss:\t\t\t1.982592\n","  test accuracy:\t\t47.39 %\n","  Epoch: 111 loss:1.973307\tAccuracy:48.38%\t lerning rate:0.01\n","  Epoch: 112 loss:1.970090\tAccuracy:48.73%\t lerning rate:0.01\n","  Epoch: 113 loss:1.971970\tAccuracy:48.45%\t lerning rate:0.01\n","  Epoch: 114 loss:1.977054\tAccuracy:47.97%\t lerning rate:0.01\n","  Epoch: 115 loss:1.976184\tAccuracy:48.12%\t lerning rate:0.01\n","  Epoch: 116 loss:1.976761\tAccuracy:48.00%\t lerning rate:0.01\n","  Epoch: 117 loss:1.977310\tAccuracy:47.98%\t lerning rate:0.01\n","  Epoch: 118 loss:1.974762\tAccuracy:48.21%\t lerning rate:0.01\n","  Epoch: 119 loss:1.969944\tAccuracy:48.75%\t lerning rate:0.01\n","  Epoch: 120 loss:1.971703\tAccuracy:48.53%\t lerning rate:0.01\n","  test loss:\t\t\t1.986891\n","  test accuracy:\t\t46.94 %\n","  Epoch: 121 loss:1.965606\tAccuracy:49.15%\t lerning rate:0.01\n","  Epoch: 122 loss:1.962300\tAccuracy:49.50%\t lerning rate:0.01\n","  Epoch: 123 loss:1.964062\tAccuracy:49.42%\t lerning rate:0.01\n","  Epoch: 124 loss:1.963490\tAccuracy:49.41%\t lerning rate:0.01\n","  Epoch: 125 loss:1.964117\tAccuracy:49.36%\t lerning rate:0.01\n","  Epoch: 126 loss:1.967192\tAccuracy:49.03%\t lerning rate:0.01\n","  Epoch: 127 loss:1.969410\tAccuracy:48.78%\t lerning rate:0.01\n","  Epoch: 128 loss:1.973542\tAccuracy:48.40%\t lerning rate:0.01\n","  Epoch: 129 loss:1.972909\tAccuracy:48.38%\t lerning rate:0.01\n","  Epoch: 130 loss:1.977582\tAccuracy:47.93%\t lerning rate:0.01\n","  test loss:\t\t\t1.988555\n","  test accuracy:\t\t46.76 %\n","  Epoch: 131 loss:1.974800\tAccuracy:48.20%\t lerning rate:0.01\n","  Epoch: 132 loss:1.973090\tAccuracy:48.38%\t lerning rate:0.01\n","  Epoch: 133 loss:1.973581\tAccuracy:48.42%\t lerning rate:0.01\n","  Epoch: 134 loss:1.973232\tAccuracy:48.44%\t lerning rate:0.01\n","  Epoch: 135 loss:1.966623\tAccuracy:49.11%\t lerning rate:0.01\n","  Epoch: 136 loss:1.964353\tAccuracy:49.35%\t lerning rate:0.01\n","  Epoch: 137 loss:1.967415\tAccuracy:48.95%\t lerning rate:0.01\n","  Epoch: 138 loss:1.972081\tAccuracy:48.49%\t lerning rate:0.01\n","  Epoch: 139 loss:1.972799\tAccuracy:48.50%\t lerning rate:0.01\n","  Epoch: 140 loss:1.962699\tAccuracy:49.45%\t lerning rate:0.01\n","  test loss:\t\t\t1.980554\n","  test accuracy:\t\t47.69 %\n","  Epoch: 141 loss:1.966833\tAccuracy:49.08%\t lerning rate:0.01\n","  Epoch: 142 loss:1.971866\tAccuracy:48.53%\t lerning rate:0.01\n","  Epoch: 143 loss:1.970487\tAccuracy:48.60%\t lerning rate:0.01\n","  Epoch: 144 loss:1.971425\tAccuracy:48.66%\t lerning rate:0.01\n","  Epoch: 145 loss:1.966502\tAccuracy:49.15%\t lerning rate:0.01\n","  Epoch: 146 loss:1.962468\tAccuracy:49.47%\t lerning rate:0.01\n","  Epoch: 147 loss:1.961707\tAccuracy:49.57%\t lerning rate:0.01\n"],"name":"stdout"}]},{"metadata":{"id":"X-tuDB0w8Lga","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"rYggDDOxzGNZ","colab_type":"code","colab":{}},"cell_type":"code","source":["!mv log_train.txt log_train_aug.txt\n","!mv log_test.txt log_test_aug.txt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m-ZOnbqDECy6","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","train1_lines = [l.strip().split() for l in open(\"log_train_noaug.txt\", \"r\")]\n","train2_lines = [l.strip().split() for l in open(\"log_train_aug.txt\", \"r\")]\n","x =[]\n","train1_error= []\n","train1_acc =[]\n","train2_error= []\n","train2_acc =[]\n","for l in zip(train1_lines, train2_lines):\n","  x.append(int(l[0][0]) )\n","  train1_error.append(float(l[0][1]) )\n","  train1_acc.append(float(l[0][2]) )\n","  train2_error.append(float(l[1][1]) )\n","  train2_acc.append(float(l[1][2]) )\n","  \n","# 表示関連\n","fig=plt.figure()\n","\n","ax=fig.add_subplot(121)\n","ax.plot(x, train1_error,color='Blue')\n","ax.plot(x, train2_error,color='Red')\n","\n","ax=fig.add_subplot(122)\n","ax.plot(x, train1_acc,color='Blue')\n","ax.plot(x, train2_acc,color='Red')\n","\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TO2fU6aw2MY6","colab_type":"code","colab":{}},"cell_type":"code","source":["test1_lines = [l.strip().split() for l in open(\"log_test_noaug.txt\", \"r\")]\n","test2_lines = [l.strip().split() for l in open(\"log_test_aug.txt\", \"r\")]\n","x =[]\n","test1_error= []\n","test1_acc =[]\n","test2_error= []\n","test2_acc =[]\n","for l in zip(test1_lines, test2_lines):\n","  x.append(int(l[0][0]) )\n","  test1_error.append(float(l[0][1]) )\n","  test1_acc.append(float(l[0][2]) )\n","  test2_error.append(float(l[1][1]) )\n","  test2_acc.append(float(l[1][2]) )\n","  \n","# 表示関連\n","fig=plt.figure()\n","\n","ax=fig.add_subplot(121)\n","ax.plot(x, test1_error,color='Blue')\n","ax.plot(x, test2_error,color='Red')\n","\n","ax=fig.add_subplot(122)\n","ax.plot(x, test1_acc,color='Blue')\n","ax.plot(x, test2_acc,color='Red')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HiPCQnEO5HN5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"BaRewbjowGkx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1802},"outputId":"b9dc3562-fb8c-497c-e7c4-3c41860ff4df","executionInfo":{"status":"ok","timestamp":1534476483325,"user_tz":-540,"elapsed":1544290,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["import tensorflow as tf\n","import pickle\n","import sys\n","import os\n","import time\n","import numpy as np\n","import glob\n","import cv2\n","\n","def load_dataset(dirpath='cifar-10-batches-py'):\n","    X, y = [], []\n","    # take data from the data batch\n","    for path in glob.glob('%s/data_batch_*' % dirpath):\n","        with open(path, 'rb') as f:\n","            batch = pickle.load(f, encoding='bytes')\n","        # append all data and labels from the 5 data betch\n","        X.append(batch[b'data'])\n","        y.append(batch[b'labels'])\n","    # devide by 255 for making value 0 to 1\n","    X = np.concatenate(X) /np.float32(255)\n","    # making labels as int\n","    y = np.concatenate(y).astype(np.int64)\n","    #seperate in to RGB colors\n","    X = np.dstack((X[:, :1024], X[:, 1024:2048], X[:, 2048:]))\n","    # reshape data into 4D tensor with compatible to CNN model\n","    X_train = X.reshape((X.shape[0], 32, 32, 3))\n","    y_train = y.reshape((y.shape[0]))\n","        \n","    # load test set\n","    path = '%s/test_batch' % dirpath\n","    with open(path, 'rb') as f:\n","        batch = pickle.load(f, encoding='bytes')\n","    X_test = batch[b'data'] /np.float32(255)\n","    X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:]))\n","    X_test = X_test.reshape((X_test.shape[0], 32, 32, 3))\n","    y_t = np.array(batch[b'labels'], dtype=np.int64)\n","    y_test = y_t.reshape((y_t.shape[0]))\n","\n","    # normalize to zero mean and unity variance\n","    offset = np.mean(X_train, 0)\n","    scale = np.std(X_train, 0).clip(min=1)\n","    X_train = (X_train - offset) / scale\n","    X_test = (X_test - offset) / scale\n","    return X_train, y_train, X_test, y_test\n","\n","# this function is used as divide input data and labels in mini batch(batchsize) and also used shuffle to give some randomness to CNN \n","def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n","    assert len(inputs) == len(targets)\n","    # shuffle is used in train the data\n","    if shuffle:\n","        indices = np.arange(len(inputs))\n","        np.random.shuffle(indices)\n","    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n","        if shuffle:\n","            excerpt = indices[start_idx:start_idx + batchsize]\n","        else:\n","            excerpt = slice(start_idx, start_idx + batchsize)\n","        yield inputs[excerpt], targets[excerpt]\n","\n","# Convolution neural network model\n","# {conv(with relu) -> max_pool -> conv(with relu) -> max_pool -> conv(with relu) -> max_pool -> dense layer -> [output(train), softmax(main predictionss)]} \n","def build_model(input_val,w,b):\n","\n","    conv1 = tf.nn.conv2d(input_val,w['w1'],strides = [1,1,1,1], padding = 'SAME')\n","    conv1 = tf.nn.bias_add(conv1,b['b1'])\n","    conv1 = tf.nn.relu(conv1)\n","    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    conv2 = tf.nn.conv2d(pool1,w['w2'],strides = [1,1,1,1], padding = 'SAME')\n","    conv2 = tf.nn.bias_add(conv2,b['b2'])\n","    conv2 = tf.nn.relu(conv2)\n","    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    conv3 = tf.nn.conv2d(pool2,w['w3'],strides = [1,1,1,1], padding = 'SAME')\n","    conv3 = tf.nn.bias_add(conv3,b['b3'])\n","    conv3 = tf.nn.relu(conv3)  \n","    pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    shape = pool3.get_shape().as_list()\n","    dense = tf.reshape(pool3,[-1,shape[1]*shape[2]*shape[3]])\n","    dense1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(dense,w['w4']),b['b4']))\n","    \n","    # used for training the CNN model\n","    out = tf.nn.bias_add(tf.matmul(dense1,w['w5']),b['b5'])\n","\n","    # used after training the CNN\n","    softmax = tf.nn.softmax(out)\n","    \n","#    return out\n","    return out,softmax\n","\n","# main function where network train and predict the output on random image\n","def main_function(num_epochs=100):\n","    \n","    # initialize input data shape and datatype for data and labels\n","    x = tf.placeholder(tf.float32,[None,32,32,3])\n","    y = tf.placeholder(tf.int64,[None])\n","    \n","    # initialize weights for every different layers\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([5,5,3,120],stddev = 0.1)),\n","        'w2': tf.Variable(tf.random_normal([5,5,120,60],stddev = 0.1)),\n","        'w3': tf.Variable(tf.random_normal([4,4,60,30],stddev = 0.1)),\n","        'w4': tf.Variable(tf.random_normal([4*4*30,30],stddev = 0.1)),\n","        'w5': tf.Variable(tf.random_normal([30,10],stddev = 0.1))\n","    }\n","\n","    # initialize biases for every different layers\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([120],stddev = 0.1)),\n","        'b2': tf.Variable(tf.random_normal([60],stddev = 0.1)),\n","        'b3': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n","        'b4': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n","        'b5': tf.Variable(tf.random_normal([10],stddev = 0.1))\n","    }\n","\n","    # call model \n","    predict,  softmax_out= build_model(x,weights,biases)\n","    # whole back propagetion process\n","    error = tf.reduce_mean( tf.losses.sparse_softmax_cross_entropy(logits = predict,labels = y))\n","    optm = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(error)\n","#    corr = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))\n","#    accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n","    \n","    corr = tf.equal(tf.argmax(predict, 1), y)\n","    accuracy = tf.reduce_mean(tf.cast(corr, tf.float32)) \n","    \n","    # initialize tensorflow session\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","    # load dataset \n","    print(\"loading dataset...\")\n","    X_train,y_train, X_test,y_test = load_dataset()\n","    # training will start\n","    print(\"Starting training...\")\n","    \n","    for epoch in range(num_epochs):\n","        train_err = 0\n","        train_acc = 0\n","        train_batches = 0\n","        start_time = time.time()\n","        # devide data into mini batch\n","        for batch in iterate_minibatches(X_train, y_train, 128, shuffle=True):\n","            # this is update weights\n","            tt = sess.run(optm,feed_dict = {x: batch[0],y: batch[1]})\n","            # cost function\n","            err,acc= sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})\n","            \n","            train_err += err\n","            train_acc += acc\n","            train_batches += 1\n","        print(\"Epoch: \",  epoch, \" Error: \", train_err/train_batches, \" Accuracy: \", train_acc/train_batches )    \n","        \n","    # testing using test dataset as per above    \n","    test_err = 0\n","    test_acc = 0\n","    test_batches = 0\n","    for batch in iterate_minibatches(X_test, y_test, 128, shuffle=False):\n","        err, acc = sess.run([error,accuracy],feed_dict = {x: batch[0],y: batch[1]})# apply tensor function\n","        test_err += err\n","        test_acc += acc\n","        test_batches += 1\n","    print(\"Final results:\")\n","    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n","    print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n","\n","    sess.close()\n","\n","main_function()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading dataset...\n","Starting training...\n","Epoch:  0  Error:  1.769444672877972  Accuracy:  0.32646233974358974\n","Epoch:  1  Error:  1.3689732319269425  Accuracy:  0.4936298076923077\n","Epoch:  2  Error:  1.238407427072525  Accuracy:  0.5477163461538461\n","Epoch:  3  Error:  1.1604523032139509  Accuracy:  0.5821514423076923\n","Epoch:  4  Error:  1.1072061511186453  Accuracy:  0.6032051282051282\n","Epoch:  5  Error:  1.0733148406713438  Accuracy:  0.614803685897436\n","Epoch:  6  Error:  1.0450280990356053  Accuracy:  0.6280849358974359\n","Epoch:  7  Error:  1.0189929687059842  Accuracy:  0.6352363782051282\n","Epoch:  8  Error:  0.997243180947426  Accuracy:  0.6438301282051282\n","Epoch:  9  Error:  0.9766303728788327  Accuracy:  0.6540665064102564\n","Epoch:  10  Error:  0.9616725834516379  Accuracy:  0.6582932692307693\n","Epoch:  11  Error:  0.9454503698226733  Accuracy:  0.6639623397435898\n","Epoch:  12  Error:  0.9336168417563805  Accuracy:  0.669110576923077\n","Epoch:  13  Error:  0.906637488420193  Accuracy:  0.6774038461538462\n","Epoch:  14  Error:  0.8889791945616404  Accuracy:  0.6863982371794872\n","Epoch:  15  Error:  0.8835817648814275  Accuracy:  0.6891426282051282\n","Epoch:  16  Error:  0.862743935523889  Accuracy:  0.6951522435897436\n","Epoch:  17  Error:  0.8616339753835629  Accuracy:  0.6938701923076923\n","Epoch:  18  Error:  0.8390749755578163  Accuracy:  0.7030649038461538\n","Epoch:  19  Error:  0.8337415138880412  Accuracy:  0.7053685897435897\n","Epoch:  20  Error:  0.8147758143070417  Accuracy:  0.7133413461538461\n","Epoch:  21  Error:  0.8057124461883154  Accuracy:  0.7170873397435897\n","Epoch:  22  Error:  0.796050870571381  Accuracy:  0.721854967948718\n","Epoch:  23  Error:  0.7804300609307412  Accuracy:  0.7233774038461539\n","Epoch:  24  Error:  0.7672386210698348  Accuracy:  0.7292267628205128\n","Epoch:  25  Error:  0.7574407164867107  Accuracy:  0.7329927884615385\n","Epoch:  26  Error:  0.7549340981703538  Accuracy:  0.7332131410256411\n","Epoch:  27  Error:  0.738460341325173  Accuracy:  0.7409455128205128\n","Epoch:  28  Error:  0.7089133600393931  Accuracy:  0.7487780448717949\n","Epoch:  29  Error:  0.7007550470339946  Accuracy:  0.7546474358974359\n","Epoch:  30  Error:  0.6870824019114177  Accuracy:  0.7594551282051282\n","Epoch:  31  Error:  0.681804532958911  Accuracy:  0.7625801282051282\n","Epoch:  32  Error:  0.6784676081094987  Accuracy:  0.7636418269230769\n","Epoch:  33  Error:  0.658806736041338  Accuracy:  0.7696714743589743\n","Epoch:  34  Error:  0.6386261474627715  Accuracy:  0.7753205128205128\n","Epoch:  35  Error:  0.6345247982404171  Accuracy:  0.7791466346153846\n","Epoch:  36  Error:  0.6331060641851181  Accuracy:  0.7786858974358974\n","Epoch:  37  Error:  0.619601086469797  Accuracy:  0.7831129807692307\n","Epoch:  38  Error:  0.6090315507772641  Accuracy:  0.7887820512820513\n","Epoch:  39  Error:  0.5980534855371866  Accuracy:  0.7919471153846154\n","Epoch:  40  Error:  0.5831105295664225  Accuracy:  0.8008012820512821\n","Epoch:  41  Error:  0.5772368177389487  Accuracy:  0.8012019230769231\n","Epoch:  42  Error:  0.5672097015075195  Accuracy:  0.8041666666666667\n","Epoch:  43  Error:  0.5575462555273986  Accuracy:  0.8084935897435898\n","Epoch:  44  Error:  0.5417558636420813  Accuracy:  0.8118990384615384\n","Epoch:  45  Error:  0.5384550692179264  Accuracy:  0.8160857371794872\n","Epoch:  46  Error:  0.5301502691629606  Accuracy:  0.8176081730769231\n","Epoch:  47  Error:  0.5234669014429435  Accuracy:  0.8189503205128205\n","Epoch:  48  Error:  0.5249067114713865  Accuracy:  0.8190504807692308\n","Epoch:  49  Error:  0.5190472044241734  Accuracy:  0.8219951923076924\n","Epoch:  50  Error:  0.5010738251300958  Accuracy:  0.8279847756410257\n","Epoch:  51  Error:  0.49125372163760356  Accuracy:  0.8318309294871795\n","Epoch:  52  Error:  0.5058331881578152  Accuracy:  0.826542467948718\n","Epoch:  53  Error:  0.4940953934421906  Accuracy:  0.8321915064102564\n","Epoch:  54  Error:  0.48726363724622973  Accuracy:  0.8344951923076923\n","Epoch:  55  Error:  0.4768664633616423  Accuracy:  0.8390024038461539\n","Epoch:  56  Error:  0.46963190803161037  Accuracy:  0.8419471153846154\n","Epoch:  57  Error:  0.46387929893456975  Accuracy:  0.8422676282051282\n","Epoch:  58  Error:  0.47360088351445323  Accuracy:  0.8387219551282051\n","Epoch:  59  Error:  0.4559623846640954  Accuracy:  0.8454727564102564\n","Epoch:  60  Error:  0.44601028301776985  Accuracy:  0.8490384615384615\n","Epoch:  61  Error:  0.45253352240110056  Accuracy:  0.8474559294871795\n","Epoch:  62  Error:  0.43024230775160666  Accuracy:  0.8557291666666667\n","Epoch:  63  Error:  0.4347351058553427  Accuracy:  0.8544070512820513\n","Epoch:  64  Error:  0.42172542000428226  Accuracy:  0.8568709935897436\n","Epoch:  65  Error:  0.44512734321447517  Accuracy:  0.8501802884615385\n","Epoch:  66  Error:  0.4080310747027397  Accuracy:  0.8635016025641026\n","Epoch:  67  Error:  0.4161280801663032  Accuracy:  0.8601963141025641\n","Epoch:  68  Error:  0.394622025008385  Accuracy:  0.867568108974359\n","Epoch:  69  Error:  0.42749692018215474  Accuracy:  0.8579126602564102\n","Epoch:  70  Error:  0.4051941596162625  Accuracy:  0.8660857371794872\n","Epoch:  71  Error:  0.3949237246926014  Accuracy:  0.8677083333333333\n","Epoch:  72  Error:  0.4065069561203321  Accuracy:  0.8664863782051282\n","Epoch:  73  Error:  0.38566930446869285  Accuracy:  0.87109375\n","Epoch:  74  Error:  0.3962967785887229  Accuracy:  0.8680288461538461\n","Epoch:  75  Error:  0.3863432471186687  Accuracy:  0.8717748397435897\n","Epoch:  76  Error:  0.3808893907910738  Accuracy:  0.8747596153846153\n","Epoch:  77  Error:  0.3770852492405818  Accuracy:  0.873417467948718\n","Epoch:  78  Error:  0.38123110013130385  Accuracy:  0.8754407051282052\n","Epoch:  79  Error:  0.3918188071021667  Accuracy:  0.8711738782051283\n","Epoch:  80  Error:  0.3637011889845897  Accuracy:  0.880048076923077\n","Epoch:  81  Error:  0.38073414407479456  Accuracy:  0.8740184294871794\n","Epoch:  82  Error:  0.3529557475676903  Accuracy:  0.8838541666666667\n","Epoch:  83  Error:  0.358144764143687  Accuracy:  0.8816105769230769\n","Epoch:  84  Error:  0.3564734568580603  Accuracy:  0.88359375\n","Epoch:  85  Error:  0.36597550335602885  Accuracy:  0.8783052884615384\n","Epoch:  86  Error:  0.36541931105729863  Accuracy:  0.8810697115384616\n","Epoch:  87  Error:  0.3656051702606372  Accuracy:  0.88046875\n","Epoch:  88  Error:  0.3609372487817055  Accuracy:  0.8799679487179487\n","Epoch:  89  Error:  0.34461074941433395  Accuracy:  0.8871394230769231\n","Epoch:  90  Error:  0.3523518912685223  Accuracy:  0.8857972756410256\n","Epoch:  91  Error:  0.3434756905604631  Accuracy:  0.8883613782051282\n","Epoch:  92  Error:  0.34068798415171797  Accuracy:  0.8899238782051282\n","Epoch:  93  Error:  0.3455251123278569  Accuracy:  0.8874399038461539\n","Epoch:  94  Error:  0.33932365018587846  Accuracy:  0.8892828525641026\n","Epoch:  95  Error:  0.33071855360116714  Accuracy:  0.8929286858974359\n","Epoch:  96  Error:  0.3490360273000522  Accuracy:  0.8856169871794872\n","Epoch:  97  Error:  0.3379256797524599  Accuracy:  0.8906049679487179\n","Epoch:  98  Error:  0.3404667032070649  Accuracy:  0.8891626602564102\n","Epoch:  99  Error:  0.37316364023165827  Accuracy:  0.8795472756410256\n","Final results:\n","  test loss:\t\t\t1.897584\n","  test accuracy:\t\t59.11 %\n"],"name":"stdout"}]},{"metadata":{"id":"VHuUy6I42pPh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"af3c4f65-0c9c-44db-dfb9-9ceb0e423484","executionInfo":{"status":"ok","timestamp":1534341299879,"user_tz":-540,"elapsed":2050,"user":{"displayName":"Takayoshi Yamashita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109048018733476271678"}}},"cell_type":"code","source":["!ls cifar-10-batches-py\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["batches.meta  data_batch_2  data_batch_4  readme.html\r\n","data_batch_1  data_batch_3  data_batch_5  test_batch\r\n"],"name":"stdout"}]},{"metadata":{"id":"y8t01ph32oGr","colab_type":"text"},"cell_type":"markdown","source":[""]}]}